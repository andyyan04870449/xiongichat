"""æŸ¥çœ‹å¢å¼·æ—¥èªŒçš„è…³æœ¬"""

import json
import sys
from datetime import datetime
from pathlib import Path


def analyze_log_file(log_file):
    """åˆ†ææ—¥èªŒæª”æ¡ˆä¸¦é¡¯ç¤ºçµ±è¨ˆ"""
    
    print(f"\nğŸ“Š åˆ†ææ—¥èªŒæª”æ¡ˆ: {log_file.name}")
    print("="*60)
    
    # çµ±è¨ˆå„éšæ®µå‡ºç¾æ¬¡æ•¸å’Œå¹³å‡è€—æ™‚
    stage_stats = {}
    workflow_times = []
    routing_stats = {"simple": 0, "moderate": 0, "complex": 0}
    crisis_levels = {"none": 0, "low": 0, "medium": 0, "high": 0}
    
    with open(log_file, 'r', encoding='utf-8') as f:
        for line in f:
            if 'éšæ®µè€—æ™‚:' in line:
                # æ‰¾åˆ°ä¸€å€‹å®Œæ•´çš„å·¥ä½œæµç¨‹
                continue
                
            if 'è·¯ç”±æ±ºç­–' in line and 'ms]' in line:
                # æå–è·¯ç”±æ±ºç­–
                if 'simple' in line:
                    routing_stats['simple'] += 1
                elif 'moderate' in line:
                    routing_stats['moderate'] += 1
                elif 'complex' in line:
                    routing_stats['complex'] += 1
                    
            if 'å±æ©Ÿè©•ä¼°' in line or 'å±æ©Ÿç­‰ç´š' in line:
                # æå–å±æ©Ÿç­‰ç´š
                for level in crisis_levels.keys():
                    if level in line:
                        crisis_levels[level] += 1
                        break
                        
            if 'ç¸½è™•ç†æ™‚é–“:' in line:
                # æå–ç¸½è™•ç†æ™‚é–“
                try:
                    time_str = line.split('ç¸½è™•ç†æ™‚é–“:')[1].split('ç§’')[0].strip()
                    workflow_times.append(float(time_str))
                except:
                    pass
    
    # é¡¯ç¤ºçµ±è¨ˆçµæœ
    print("\nğŸ”€ è·¯ç”±æ±ºç­–åˆ†å¸ƒ:")
    total_routes = sum(routing_stats.values())
    for route, count in routing_stats.items():
        percentage = (count/total_routes*100) if total_routes > 0 else 0
        print(f"  {route}: {count} ({percentage:.1f}%)")
    
    print("\nğŸš¨ å±æ©Ÿç­‰ç´šåˆ†å¸ƒ:")
    total_crisis = sum(crisis_levels.values())
    for level, count in crisis_levels.items():
        percentage = (count/total_crisis*100) if total_crisis > 0 else 0
        icon = {"none": "âœ…", "low": "ğŸŸ¡", "medium": "ğŸŸ ", "high": "ğŸ”´"}.get(level, "â“")
        print(f"  {icon} {level}: {count} ({percentage:.1f}%)")
    
    print("\nâ±ï¸ è™•ç†æ™‚é–“çµ±è¨ˆ:")
    if workflow_times:
        print(f"  å¹³å‡: {sum(workflow_times)/len(workflow_times):.2f}ç§’")
        print(f"  æœ€å¿«: {min(workflow_times):.2f}ç§’")
        print(f"  æœ€æ…¢: {max(workflow_times):.2f}ç§’")
        print(f"  ç¸½è¨ˆ: {len(workflow_times)} å€‹è«‹æ±‚")


def show_recent_entries(log_file, n=5):
    """é¡¯ç¤ºæœ€è¿‘çš„æ—¥èªŒæ¢ç›®"""
    
    print(f"\nğŸ“ æœ€è¿‘ {n} å€‹è«‹æ±‚æ‘˜è¦:")
    print("="*60)
    
    with open(log_file, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    # æ‰¾å‡ºè«‹æ±‚é–‹å§‹çš„ä½ç½®
    request_starts = []
    for i, line in enumerate(lines):
        if 'ğŸš€ æ–°å°è©±è«‹æ±‚' in line:
            request_starts.append(i)
    
    # é¡¯ç¤ºæœ€è¿‘çš„è«‹æ±‚
    for req_idx in request_starts[-n:]:
        if req_idx < len(lines):
            # æå–è«‹æ±‚è³‡è¨Š
            print("\n" + lines[req_idx-1].strip())  # åˆ†éš”ç·š
            for i in range(req_idx, min(req_idx + 50, len(lines))):
                line = lines[i]
                if 'ç”¨æˆ¶:' in line or 'è¨Šæ¯:' in line:
                    print(line.strip())
                elif 'è·¯ç”±æ±ºç­–' in line:
                    print(line.strip())
                elif 'å±æ©Ÿ' in line and 'ç­‰ç´š' in line:
                    print(line.strip())
                elif 'æœ€çµ‚å›æ‡‰' in line:
                    print(line.strip())
                    # é¡¯ç¤ºå›æ‡‰å…§å®¹ï¼ˆé™åˆ¶é•·åº¦ï¼‰
                    if i+1 < len(lines):
                        response = lines[i+1].strip()
                        if len(response) > 100:
                            response = response[:100] + "..."
                        print(f"  å›æ‡‰: {response}")
                elif 'ç¸½è™•ç†æ™‚é–“' in line:
                    print(line.strip())
                    break


def analyze_jsonl_file(jsonl_file):
    """åˆ†æ JSONL æ ¼å¼çš„è©³ç´°æ—¥èªŒ"""
    
    print(f"\nğŸ“ˆ è©³ç´°æ•¸æ“šåˆ†æ: {jsonl_file.name}")
    print("="*60)
    
    stage_timings = {}
    llm_calls = 0
    
    with open(jsonl_file, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                data = json.loads(line)
                stage = data.get('stage', '')
                
                # çµ±è¨ˆéšæ®µè€—æ™‚
                if 'stage_time_ms' in data:
                    if stage not in stage_timings:
                        stage_timings[stage] = []
                    stage_timings[stage].append(data['stage_time_ms'])
                
                # çµ±è¨ˆ LLM èª¿ç”¨
                if stage in ['ROUTING_DECISION', 'COMBINED_ANALYSIS', 'CONTEXT_UNDERSTANDING', 
                           'SEMANTIC_ANALYSIS', 'INTENT_ROUTING', 'RESPONSE_GENERATION']:
                    llm_calls += 1
                    
            except json.JSONDecodeError:
                continue
    
    print("\nâš™ï¸ å„éšæ®µå¹³å‡è€—æ™‚:")
    for stage, times in sorted(stage_timings.items()):
        if times:
            avg_time = sum(times) / len(times)
            print(f"  {stage}: {avg_time:.0f}ms (n={len(times)})")
    
    print(f"\nğŸ¤– LLM èª¿ç”¨æ¬¡æ•¸: {llm_calls}")


def main():
    """ä¸»å‡½æ•¸"""
    
    # å–å¾—ä»Šå¤©çš„æ—¥èªŒæª”æ¡ˆ
    log_dir = Path("logs/ai_analysis")
    today = datetime.now().strftime("%Y%m%d")
    
    log_file = log_dir / f"ai_analysis_{today}.log"
    jsonl_file = log_dir / f"ai_analysis_{today}.jsonl"
    
    if log_file.exists():
        analyze_log_file(log_file)
        show_recent_entries(log_file, n=3)
    else:
        print(f"âŒ æ‰¾ä¸åˆ°æ—¥èªŒæª”æ¡ˆ: {log_file}")
    
    if jsonl_file.exists():
        analyze_jsonl_file(jsonl_file)
    else:
        print(f"âŒ æ‰¾ä¸åˆ° JSONL æª”æ¡ˆ: {jsonl_file}")


if __name__ == "__main__":
    main()