"""æ¥µç°¡å·¥ä½œæµ - 3æ­¥é©Ÿæ¶æ§‹ï¼Œæ™ºèƒ½é›†ä¸­åœ¨æœ€çµ‚LLM"""

import asyncio
import json
import logging
import time
from typing import Dict, Any, List, Optional
from datetime import datetime
import uuid

from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage, AIMessage
from cachetools import TTLCache

from app.config import settings
from app.langgraph.state import WorkflowState
from app.services.rag_retriever import RAGRetriever
from app.utils.ai_logger import get_ai_logger
from app.utils.ultimate_logger import get_ultimate_logger
from app.utils.quality_logger import get_quality_logger
# ç§»é™¤å­—æ•¸é™åˆ¶ç®¡ç†å™¨import
from app.langgraph.nodes.conversation_logger import ConversationLoggerNode
from app.services.enhanced_memory import EnhancedMemoryService
from app.langgraph.intent_cleaner import IntentCleaner
from app.services.google_places_service import GooglePlacesService

logger = logging.getLogger(__name__)


class IntentAnalyzer:
    """æ„åœ–åˆ†æå™¨ - åªåˆ†æä¸ç”Ÿæˆ"""
    
    ANALYSIS_PROMPT = """ä½ æ˜¯å°ˆæ¥­çš„å±æ©Ÿåµæ¸¬èˆ‡æ„åœ–åˆ†æç³»çµ±ã€‚è«‹ä»”ç´°åˆ†æç”¨æˆ¶è¼¸å…¥ï¼Œè¿”å›JSONæ ¼å¼ã€‚

# é‡è¦åˆ†æåŸå‰‡
**ç¶œåˆåˆ†æç•¶å‰ç”¨æˆ¶è¼¸å…¥èˆ‡å°è©±æ­·å²**
éœ€è¦çµåˆå°è©±æ­·å²ä¾†è©•ä¼°ç­–ç•¥æ•ˆæœå’Œå‡ç´šéœ€æ±‚ï¼ŒåŒæ™‚åŸºæ–¼ç•¶å‰è¼¸å…¥é€²è¡Œå±æ©Ÿåˆ¤æ–·ã€‚

# å±æ©Ÿè­˜åˆ¥æº–å‰‡ï¼ˆæœ€é«˜å„ªå…ˆç´šï¼‰

## æ˜ç¢ºå±æ©Ÿä¿¡è™Ÿ â†’ risk_level: "high"
**åˆ†æåŸå‰‡ï¼šé—œæ³¨èªç¾©å’Œæƒ…æ„Ÿå¼·åº¦ï¼Œä¸æ‹˜æ³¥æ–¼å…·é«”è©å½™**

- ç›´æ¥è‡ªæ®ºæ„å¿µï¼šæ˜ç¢ºè¡¨é”çµæŸç”Ÿå‘½çš„æƒ³æ³•æˆ–è¨ˆç•«
- éš±å–»æ€§è¡¨é”ï¼šæš—ç¤ºé€ƒé¿ã€æ¶ˆå¤±ã€è§£è„«ç­‰æ¦‚å¿µ
- è¡Œç‚ºæ€§æš—ç¤ºï¼šæåˆ°å±éšªå ´æ‰€ã€è‡ªå‚·å·¥å…·æˆ–è¡Œç‚º
- æ¥µåº¦çµ•æœ›ï¼šè¡¨é”å®Œå…¨å¤±å»å¸Œæœ›ã€æ„ç¾©æ„Ÿå–ªå¤±
- ä¸–ç•Œè§€å´©æ½°ï¼šèªç‚ºä¸–ç•Œ/äººç”Ÿå®Œå…¨ç„¡åƒ¹å€¼ã€ç„¡æ„ç¾©
- æƒ…æ„Ÿéº»æœ¨ï¼šè¡¨é”å®Œå…¨å¤±å»æ„Ÿå—èƒ½åŠ›æˆ–å‹•æ©Ÿ

## ä¸­åº¦é¢¨éšªä¿¡è™Ÿ â†’ risk_level: "medium"
**åˆ†æåŸå‰‡ï¼šè­˜åˆ¥è‡ªå‚·å‚¾å‘å’Œæ¥µåº¦ç—›è‹¦ï¼Œä½†æœªé”ç›´æ¥è‡ªæ®ºé¢¨éšª**

- è‡ªå‚·å‚¾å‘ï¼šè¡¨é”æƒ³è¦å‚·å®³æˆ–æ‡²ç½°è‡ªå·±çš„æƒ³æ³•
- æ¥µåº¦ç—›è‹¦ï¼šæè¿°ç„¡æ³•æ‰¿å—çš„ç—›è‹¦ï¼Œä½†æœªæ˜ç¢ºè¡¨é”æ­»äº¡æ„å¿µ
- é€ƒé¿ç¾å¯¦ï¼šå¸Œæœ›é€šéç‰©è³ªæˆ–å…¶ä»–æ–¹å¼å¾¹åº•é€ƒé¿ç¾å¯¦
- å¼·çƒˆç„¡åŠ©æ„Ÿï¼šå®Œå…¨å¤±å»æ§åˆ¶æ„Ÿå’Œæ‡‰å°èƒ½åŠ›

## ä½åº¦é¢¨éšªä¿¡è™Ÿ â†’ risk_level: "low"
**åˆ†æåŸå‰‡ï¼šè­˜åˆ¥ä¸€èˆ¬è² é¢æƒ…ç·’å’Œç”Ÿæ´»å›°æ“¾**

- æƒ…ç·’å›°æ“¾ï¼šè¡¨é”æ†‚é¬±ã€ç„¦æ…®ã€å­¤å–®ç­‰è² é¢æƒ…ç·’
- ç”Ÿæ´»å•é¡Œï¼šç¡çœ ã€é£Ÿæ…¾ã€å‹•æ©Ÿç­‰æ—¥å¸¸åŠŸèƒ½å›°æ“¾
- è¼•åº¦æ±‚åŠ©ï¼šè¡¨é”éœ€è¦å¹«åŠ©ä½†ç„¡æ€¥è¿«æ€§æˆ–å±éšªæ€§

# æ„åœ–åˆ†é¡æº–å‰‡

- "å±æ©Ÿ": risk_levelç‚ºhighæ™‚
- "æ±‚åŠ©": æ˜ç¢ºå°‹æ±‚æˆ’æ¯’ã€æ²»ç™‚ã€å¾©å¥å”åŠ©
- "è«®è©¢": è©¢å•æ©Ÿæ§‹ã€é›»è©±ã€åœ°å€ã€æœå‹™å…§å®¹
- "æƒ…ç·’æ”¯æŒ": è¡¨é”è² é¢æƒ…ç·’ã€éœ€è¦é™ªä¼´
- "å•å€™": æ—¥å¸¸æ‹›å‘¼ã€ä½ å¥½ã€æ—©å®‰ç­‰
- "ä¸€èˆ¬å°è©±": å…¶ä»–æ‰€æœ‰æƒ…æ³

# RAGè§¸ç™¼æ¢ä»¶ï¼ˆåš´æ ¼æ§åˆ¶ï¼‰
**need_rag: true åƒ…åœ¨ä»¥ä¸‹æƒ…æ³**ï¼š
- ç”¨æˆ¶æ˜ç¢ºè©¢å•æœå‹™è³‡è¨Šï¼ˆé›»è©±ã€åœ°å€ã€æ™‚é–“ã€æœå‹™å…§å®¹ï¼‰
- ç”¨æˆ¶ä¸»å‹•è¦æ±‚è¯çµ¡è³‡è¨Šæˆ–å…·é«”å¹«åŠ©
- care_stage_neededç‚º3ä¸”å‰å…©å±¤ç­–ç•¥å·²ç„¡æ•ˆ

**need_rag: false æ‰€æœ‰å…¶ä»–æƒ…æ³**ï¼š
- ç´”æƒ…ç·’è¡¨é”æˆ–å±æ©Ÿç‹€æ…‹
- ä¸€èˆ¬å°è©±æˆ–å•å€™
- å·²åœ¨é€²è¡Œæƒ…ç·’æ”¯æŒå°è©±ä¸­

# Google Places APIè§¸ç™¼æ¢ä»¶
**need_places_api: true ç•¶ç¬¦åˆä»¥ä¸‹æ¢ä»¶**ï¼š
- æ˜ç¢ºè©¢å•ç‰¹å®šæ©Ÿæ§‹çš„é›»è©±ã€åœ°å€ã€ç‡Ÿæ¥­æ™‚é–“
- æåˆ°å…·é«”çš„æ©Ÿæ§‹åç¨±ï¼ˆå¦‚ï¼šé«˜é›„å¸‚æ¯’å“é˜²åˆ¶å±€ã€æŸæŸé†«é™¢ï¼‰
- è©¢å•ã€Œåœ¨å“ªè£¡ã€ã€ã€Œæ€éº¼å»ã€ã€ã€Œè¯çµ¡æ–¹å¼ã€ç­‰

**place_query_info åˆ¤æ–·é‚è¼¯**ï¼š
- query_type: æ ¹æ“šé—œéµè©åˆ¤æ–·ï¼ˆé›»è©±â†’phone, åœ°å€/åœ¨å“ªâ†’address, ç‡Ÿæ¥­æ™‚é–“â†’hoursï¼‰
- place_name: å¾ç”¨æˆ¶è¼¸å…¥ä¸­æå–å¯èƒ½çš„æ©Ÿæ§‹åç¨±
- confidence: æ ¹æ“šé—œéµè©æ˜ç¢ºç¨‹åº¦çµ¦å‡ºä¿¡å¿ƒåˆ†æ•¸

# é—œæ‡·ç­–ç•¥å‡ç´šæ©Ÿåˆ¶
æ ¹æ“šå°è©±æ­·å²è©•ä¼°AIé—œæ‡·ç­–ç•¥çš„æœ‰æ•ˆæ€§ä¸¦æ±ºå®šå‡ç´šï¼š

## ç­–ç•¥æœ‰æ•ˆæ€§è©•ä¼°æ¨™æº–
1. **æœ‰æ•ˆæŒ‡æ¨™**ï¼š
   - ç”¨æˆ¶æƒ…ç·’ç‹€æ…‹æ”¹å–„ï¼ˆrisk_levelä¸‹é™ï¼‰
   - ç”¨æˆ¶é–‹å§‹ä¸»å‹•æºé€šæˆ–è©¢å•è³‡æº
   - å‡ºç¾æ„Ÿè¬ã€èªåŒç­‰æ­£é¢å›æ‡‰
   - é¡˜æ„åˆ†äº«æ›´å¤šå€‹äººæƒ…æ³

2. **ç„¡æ•ˆæŒ‡æ¨™**ï¼š
   - å±æ©Ÿç¨‹åº¦æŒçºŒæˆ–åŠ æ·±
   - ç”¨æˆ¶é‡è¤‡è¡¨é”ç›¸åŒç—›è‹¦
   - å°AIå›æ‡‰ç„¡åæ‡‰æˆ–è² é¢åæ‡‰
   - æ˜ç¢ºæ‹’çµ•å¹«åŠ©æˆ–è¡¨é”å¤±æœ›

## å‡ç´šè§¸ç™¼æ©Ÿåˆ¶
**è‡ªå‹•å‡ç´šæ¢ä»¶**ï¼š
- åŒä¸€å±¤ç­–ç•¥é€£çºŒä½¿ç”¨2-3æ¬¡ä»ç„¡æ”¹å–„ â†’ å‡ç´šä¸‹ä¸€å±¤
- ç”¨æˆ¶å±æ©Ÿç¨‹åº¦æ˜é¡¯åŠ æ·± â†’ è·³ç´šè™•ç†
- ç”¨æˆ¶æ˜ç¢ºè¡¨é”å°ç•¶å‰æ–¹å¼ä¸æ»¿ â†’ ç«‹å³å‡ç´š

**å‡ç´šè·¯å¾‘**ï¼š
- ç¬¬1å±¤ç„¡æ•ˆ â†’ å‡ç´šè‡³ç¬¬2å±¤ï¼ˆåŠ å¼·é™ªä¼´æ”¯æŒï¼‰
- ç¬¬2å±¤ç„¡æ•ˆ â†’ å‡ç´šè‡³ç¬¬3å±¤ï¼ˆæä¾›å…·é«”è³‡æºï¼‰  
- ç¬¬3å±¤ç„¡æ•ˆ â†’ ç‰¹æ®Šè™•ç†ï¼ˆå»ºè­°å°ˆæ¥­å¹«åŠ©ï¼‰

## ç­–ç•¥æ­·å²åˆ†æè¦é»
- çµ±è¨ˆæœ€è¿‘3-5æ¬¡å°è©±çš„ç­–ç•¥ä½¿ç”¨æƒ…æ³
- æ¯”è¼ƒç”¨æˆ¶æƒ…ç·’è®ŠåŒ–è¶¨å‹¢ï¼ˆæƒ¡åŒ–/æŒå¹³/æ”¹å–„ï¼‰
- è­˜åˆ¥"æ²»ç™‚åœæ»¯"ç‹€æ…‹ï¼ˆé‡è¤‡å¾ªç’°ç„¡é€²å±•ï¼‰
- è©•ä¼°ç”¨æˆ¶å°ä¸åŒç­–ç•¥é¡å‹çš„åæ‡‰æ¨¡å¼

# ç¯„ä¾‹åˆ†æ

è¼¸å…¥ï¼šã€Œæ´»è‘—å¥½ç´¯ï¼Œæƒ³è¦è§£è„«äº†ã€
åˆ†æï¼šéš±å–»æ€§è¡¨é”è§£è„«ï¼Œé«˜é¢¨éšª
è¼¸å‡ºï¼šrisk_level="high", intent="å±æ©Ÿ"

è¼¸å…¥ï¼šã€Œæˆ‘è¦ºå¾—æ²’æœ‰æ˜å¤©äº†ã€
åˆ†æï¼šè¡¨é”çµ•æœ›ï¼Œæš—ç¤ºçµæŸç”Ÿå‘½
è¼¸å‡ºï¼šrisk_level="high", intent="å±æ©Ÿ"

è¼¸å…¥ï¼šã€Œæˆ‘ç¾åœ¨åœ¨é ‚æ¨“ã€
åˆ†æï¼šä½ç½®æš—ç¤ºæ½›åœ¨å±éšªè¡Œç‚º
è¼¸å‡ºï¼šrisk_level="high", intent="å±æ©Ÿ"

è¼¸å…¥ï¼šã€Œæˆ‘æƒ³å‚·å®³è‡ªå·±ã€
åˆ†æï¼šæ˜ç¢ºè‡ªå‚·æ„åœ–
è¼¸å‡ºï¼šrisk_level="medium", intent="å±æ©Ÿ"

# å¯¦é«”è­˜åˆ¥
**é‡è¦**ï¼šå¾ç”¨æˆ¶è¼¸å…¥ä¸­æº–ç¢ºæå–æ©Ÿæ§‹/åœ°é»åç¨±
- ç§»é™¤æŸ¥è©¢è©ï¼ˆå¦‚ã€Œçš„é›»è©±ã€ã€ã€Œåœ¨å“ªè£¡ã€ã€ã€Œæ€éº¼å»ã€ã€ã€Œç‡Ÿæ¥­æ™‚é–“ã€ï¼‰
- åªä¿ç•™ç´”ç²¹çš„æ©Ÿæ§‹åç¨±
- ä¾‹å¦‚ï¼šã€Œå‡±æ—‹é†«é™¢åœ¨å“ªè£¡ï¼Ÿã€â†’ å¯¦é«”ç‚ºã€Œå‡±æ—‹é†«é™¢ã€
- ä¾‹å¦‚ï¼šã€Œé«˜é›„å¸‚æ¯’å“é˜²åˆ¶å±€çš„é›»è©±ã€â†’ å¯¦é«”ç‚ºã€Œé«˜é›„å¸‚æ¯’å“é˜²åˆ¶å±€ã€

# è¿”å›æ ¼å¼
{{
    "risk_level": "none/low/medium/high",
    "intent": "å•å€™/æ±‚åŠ©/è«®è©¢/å±æ©Ÿ/æƒ…ç·’æ”¯æŒ/ä¸€èˆ¬å°è©±",
    "need_rag": true/false,
    "need_places_api": true/false,
    "place_entity": "æå–çš„æ©Ÿæ§‹åç¨±ï¼ˆç´”æ·¨ï¼‰",
    "place_query_type": "phone/address/hours/general",
    "search_keywords": ["é—œéµè©1", "é—œéµè©2"],
    "entities": {{
        "institutions": ["æåˆ°çš„æ©Ÿæ§‹/é†«é™¢"],
        "substances": ["æåˆ°çš„æ¯’å“"],
        "locations": ["æåˆ°çš„åœ°é»"],
        "symptoms": ["æåˆ°çš„ç—‡ç‹€"]
    }},
    "emotional_state": "çµ•æœ›/ç„¦æ…®/å¹³éœ/ç©æ¥µ/ä¸æ˜",
    "urgency": "immediate/high/normal/low",
    "care_stage_needed": 1,
    "care_stage_reason": "éšæ®µé¸æ“‡çš„ç†ç”±èªªæ˜",
    "strategy_effectiveness": "effective/ineffective/unknown/improving",
    "upgrade_reason": "ç­–ç•¥å‡ç´šçš„å…·é«”åŸå› ",
    "previous_stages_tried": [1, 1, 2],
    "emotion_trend": "improving/stable/deteriorating/unknown",
    "treatment_progress": "initial/ongoing/stagnant/breakthrough",
    "confidence_level": 0.85
}}

ç”¨æˆ¶è¼¸å…¥ï¼š{input_text}
å°è©±æ­·å²ï¼š{memory}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å…§å®¹ã€‚"""

    def __init__(self):
        self.llm = ChatOpenAI(
            model=settings.openai_model_analysis,  # gpt-4o-mini
            temperature=0.1,
            max_tokens=400,
            api_key=settings.openai_api_key,
        )
        self.cache = TTLCache(maxsize=100, ttl=300)  # 5åˆ†é˜å¿«å–
    
    def _analyze_strategy_history(self, memory: List[Dict]) -> Dict:
        """åˆ†æç­–ç•¥ä½¿ç”¨æ­·å²å’Œæœ‰æ•ˆæ€§"""
        if not memory:
            return {
                "previous_stages_tried": [],
                "strategy_effectiveness": "unknown",
                "emotion_trend": "unknown", 
                "treatment_progress": "initial",
                "last_risk_levels": []
            }
        
        # æå–AIå›æ‡‰ä¸­çš„ç­–ç•¥ä½¿ç”¨æ¨¡å¼
        ai_responses = [msg for msg in memory if msg.get("role") == "assistant"]
        user_responses = [msg for msg in memory if msg.get("role") == "user"]
        
        # åˆ†ææœ€è¿‘ä½¿ç”¨çš„ç­–ç•¥ï¼ˆåŸºæ–¼å›æ‡‰å…§å®¹æ¨æ¸¬ï¼‰
        recent_stages = []
        last_risk_levels = []
        
        # æ”¹å–„çš„ç­–ç•¥è­˜åˆ¥é‚è¼¯ - æ›´æº–ç¢ºçš„åˆ†å±¤åˆ¤æ–·
        for i, response in enumerate(ai_responses[-5:]):  # æœ€è¿‘5æ¬¡AIå›æ‡‰
            content = response.get("content", "").lower()
            
            # ç¬¬3å±¤ç­–ç•¥é—œéµè© - å…·é«”è³‡æºæä¾›
            if any(word in content for word in ["é›»è©±", "åœ°å€", "è³‡æº", "å°ˆæ¥­", "æ©Ÿæ§‹", "072865580", "é«˜é›„å¸‚æ¯’å“é˜²åˆ¶å±€"]):
                stage = 3
            # ç¬¬1å±¤ç­–ç•¥é—œéµè© - æƒ…æ„Ÿç¢ºèªèˆ‡åŒç†
            elif any(word in content for word in ["è½èµ·ä¾†", "æ„Ÿè¦º", "ç†è§£", "è¾›è‹¦", "å¯ä»¥ç†è§£", "çœŸçš„å¾ˆ"]):
                stage = 1
            # ç¬¬2å±¤ç­–ç•¥é—œéµè© - é™ªä¼´èˆ‡æ”¯æŒ
            elif any(word in content for word in ["é™ªä¼´", "æ”¯æŒ", "é™ªè‘—ä½ ", "åœ¨é€™è£¡", "ä¸€èµ·", "éš¨æ™‚"]):
                stage = 2
            else:
                # æ ¹æ“šå°è©±è¼ªæ¬¡æ¨æ¸¬ï¼šåˆæœŸå‚¾å‘ç¬¬1å±¤ï¼Œå¾ŒæœŸç¬¬2å±¤
                if i < 2:
                    stage = 1
                else:
                    stage = 2
            
            recent_stages.append(stage)
        
        # è©•ä¼°ç­–ç•¥æœ‰æ•ˆæ€§ï¼ˆç°¡åŒ–é‚è¼¯ï¼‰
        if len(user_responses) >= 2:
            # æ¯”è¼ƒæœ€è¿‘çš„ç”¨æˆ¶å›æ‡‰ï¼Œçœ‹æ˜¯å¦æœ‰æ”¹å–„è·¡è±¡
            recent_user = user_responses[-1].get("content", "").lower()
            prev_user = user_responses[-2].get("content", "").lower()
            
            # ç°¡å–®çš„æ”¹å–„æŒ‡æ¨™
            positive_words = ["è¬è¬", "æ„Ÿè¬", "å¥½ä¸€é»", "å¹«åŠ©", "ç†è§£", "æ”¯æŒ"]
            negative_words = ["æ²’ç”¨", "ä¸è¡Œ", "æ›´ç³Ÿ", "ç„¡æ•ˆ", "å¤±æœ›", "æ”¾æ£„"]
            
            if any(word in recent_user for word in positive_words):
                effectiveness = "improving"
            elif any(word in recent_user for word in negative_words):
                effectiveness = "ineffective"
            elif recent_user == prev_user:  # é‡è¤‡ç›¸åŒå…§å®¹
                effectiveness = "ineffective"
            else:
                effectiveness = "unknown"
        else:
            effectiveness = "unknown"
        
        # æ²»ç™‚é€²ç¨‹è©•ä¼°
        if len(recent_stages) == 0:
            progress = "initial"
        elif len(set(recent_stages)) == 1 and len(recent_stages) >= 3:
            progress = "stagnant"  # åŒæ¨£ç­–ç•¥é‡è¤‡ä½¿ç”¨
        elif effectiveness == "improving":
            progress = "breakthrough"
        else:
            progress = "ongoing"
        
        return {
            "previous_stages_tried": recent_stages,
            "strategy_effectiveness": effectiveness,
            "emotion_trend": "stable",  # ç°¡åŒ–è™•ç†
            "treatment_progress": progress,
            "last_risk_levels": last_risk_levels
        }
    
    def _determine_upgrade_strategy(self, current_analysis: Dict, history_analysis: Dict) -> Dict:
        """æ±ºå®šæ˜¯å¦éœ€è¦å‡ç´šç­–ç•¥"""
        
        current_risk = current_analysis.get("risk_level", "none")
        previous_stages = history_analysis.get("previous_stages_tried", [])
        effectiveness = history_analysis.get("strategy_effectiveness", "unknown")
        progress = history_analysis.get("treatment_progress", "initial")
        
        # å‹•æ…‹ç­–ç•¥é¸æ“‡ - åŸºæ–¼æƒ…å¢ƒå’Œæ­·å²
        if current_risk == "high":
            default_stage = 1  # é«˜é¢¨éšªå¾æƒ…æ„Ÿç¢ºèªé–‹å§‹
        elif current_analysis.get("intent") == "è«®è©¢":
            default_stage = 3  # è«®è©¢ç›´æ¥æä¾›è³‡æº
        elif not previous_stages:  # é¦–æ¬¡å°è©±
            default_stage = 1  # å¾ç¬¬1å±¤é–‹å§‹
        else:
            default_stage = 2  # å…¶ä»–æƒ…æ³é è¨­ç¬¬2å±¤
        
        # å‡ç´šæ±ºç­–é‚è¼¯ - æ”¾å¯¬æ¢ä»¶
        upgrade_needed = False
        upgrade_reason = ""
        suggested_stage = default_stage
        
        if previous_stages:
            last_stage = previous_stages[-1] if previous_stages else 1
            
            # 1. ç­–ç•¥ç„¡æ•ˆæ™‚å‡ç´š (æ”¾å¯¬æ¢ä»¶)
            if effectiveness == "ineffective":
                upgrade_needed = True
                suggested_stage = min(last_stage + 1, 3)
                upgrade_reason = f"ç¬¬{last_stage}å±¤ç­–ç•¥æ•ˆæœä¸ä½³ï¼Œå‡ç´šè‡³ç¬¬{suggested_stage}å±¤"
            
            # 2. åŒå±¤ç­–ç•¥ä½¿ç”¨éå¤šæ™‚å‡ç´š
            elif len(previous_stages) >= 2 and all(s == last_stage for s in previous_stages[-2:]):
                upgrade_needed = True
                suggested_stage = min(last_stage + 1, 3)
                upgrade_reason = f"ç¬¬{last_stage}å±¤ç­–ç•¥é‡è¤‡ä½¿ç”¨ï¼Œå‡ç´šè‡³ç¬¬{suggested_stage}å±¤"
            
            # 3. æ²»ç™‚åœæ»¯æ™‚å‡ç´š
            elif progress == "stagnant":
                upgrade_needed = True
                most_used_stage = max(set(previous_stages), key=previous_stages.count)
                suggested_stage = min(most_used_stage + 1, 3)
                upgrade_reason = f"æ²»ç™‚é€²å±•åœæ»¯ï¼Œå¾ç¬¬{most_used_stage}å±¤å‡ç´šè‡³ç¬¬{suggested_stage}å±¤"
            
            # 4. é¢¨éšªç¨‹åº¦è®ŠåŒ–èª¿æ•´
            elif current_risk == "high" and last_stage > 1:
                upgrade_needed = True
                suggested_stage = 1
                upgrade_reason = "ç”¨æˆ¶å±æ©Ÿç¨‹åº¦åŠ æ·±ï¼Œå›åˆ°ç¬¬1å±¤æƒ…æ„Ÿç¢ºèª"
            
            # 5. æƒ…ç·’æ˜é¡¯æƒ¡åŒ–æ™‚å‡ç´š
            elif current_analysis.get("emotional_state") in ["çµ•æœ›", "æ†¤æ€’"] and last_stage < 3:
                upgrade_needed = True
                suggested_stage = min(last_stage + 1, 3)
                upgrade_reason = f"ç”¨æˆ¶æƒ…ç·’æƒ¡åŒ–ï¼Œå¾ç¬¬{last_stage}å±¤å‡ç´šè‡³ç¬¬{suggested_stage}å±¤"
        
        if not upgrade_needed:
            suggested_stage = default_stage
            upgrade_reason = "ç¶­æŒç•¶å‰ç­–ç•¥å±¤æ¬¡"
        
        return {
            "care_stage_needed": suggested_stage,
            "upgrade_reason": upgrade_reason,
            "is_upgrade": upgrade_needed
        }

    async def analyze(self, input_text: str, memory: List[Dict] = None) -> Dict:
        """åˆ†æç”¨æˆ¶æ„åœ–"""

        # æª¢æŸ¥å¿«å–
        cache_key = f"intent:{input_text[:50]}"
        if cache_key in self.cache:
            logger.info(f"Intent cache hit for: {input_text[:30]}")
            return self.cache[cache_key]

        try:
            # ä½¿ç”¨PlaceQueryDetectoræª¢æ¸¬åœ°é»æŸ¥è©¢
            from app.services.google_places_service import PlaceQueryDetector
            place_detection = PlaceQueryDetector.detect_place_query(input_text)

            # æ ¼å¼åŒ–è¨˜æ†¶
            memory_str = self._format_memory(memory) if memory else "ç„¡"

            # æ§‹å»ºæç¤º
            prompt = self.ANALYSIS_PROMPT.format(
                input_text=input_text,
                memory=memory_str
            )

            messages = [
                SystemMessage(content="ä½ æ˜¯å°ˆæ¥­çš„å°è©±åˆ†æç³»çµ±ï¼Œåªè¿”å›JSONæ ¼å¼çµæœã€‚"),
                HumanMessage(content=prompt)
            ]

            # åŸ·è¡Œåˆ†æ
            response = await self.llm.ainvoke(messages)

            # è§£æçµæœ - å¢å¼·JSONè™•ç†
            try:
                result = self._parse_json_response(response.content)
                if result:
                    # ä½¿ç”¨LLMè¿”å›çš„å¯¦é«”å’ŒæŸ¥è©¢é¡å‹
                    if not result.get("place_entity"):
                        # å¦‚æœLLMæ²’æœ‰æå–å¯¦é«”ï¼Œä½¿ç”¨PlaceQueryDetectorä½œç‚ºå‚™ä»½
                        result["place_entity"] = place_detection.get("place_name", "")

                    if not result.get("place_query_type"):
                        result["place_query_type"] = place_detection.get("query_type", "general")

                    # æ ¹æ“šæ˜¯å¦æœ‰å¯¦é«”ä¾†æ±ºå®šæ˜¯å¦éœ€è¦Places API
                    result["need_places_api"] = bool(result.get("place_entity"))

                    # é€²è¡Œç­–ç•¥æ­·å²åˆ†æå’Œå‡ç´šæ±ºç­–
                    history_analysis = self._analyze_strategy_history(memory)
                    upgrade_decision = self._determine_upgrade_strategy(result, history_analysis)

                    # æ•´åˆå‡ç´šæ±ºç­–åˆ°çµæœä¸­
                    result.update(upgrade_decision)
                    result.update(history_analysis)

                    # åŠ å…¥å¿«å–
                    self.cache[cache_key] = result
                    logger.info(f"[Intent Analysis] âœ… æˆåŠŸè§£æ - risk={result.get('risk_level')}, intent={result.get('intent')}, stage={result.get('care_stage_needed')}, places_api={result.get('need_places_api')}")
                    logger.debug(f"[Intent Analysis] å®Œæ•´çµæœ: {json.dumps(result, ensure_ascii=False, indent=2)}")
                    return result
                else:
                    logger.warning(f"[Intent Analysis] âŒ JSONæ ¼å¼ç„¡æ•ˆ: {response.content[:200]}")
                    logger.debug(f"[Intent Analysis] åŸå§‹LLMå›æ‡‰: {response.content[:1000]}")
                    return self._get_default_analysis(input_text)
            except Exception as parse_error:
                logger.error(f"[Intent Analysis] âŒ JSONè§£æå¤±æ•—: {str(parse_error)}")
                logger.debug(f"[Intent Analysis] åŸå§‹å›æ‡‰å…§å®¹: {response.content}")
                return self._get_default_analysis(input_text)
                
        except Exception as e:
            logger.error(f"Intent analysis error: {str(e)}")
            logger.error(f"Exception type: {type(e).__name__}")
            if hasattr(e, '__traceback__'):
                import traceback
                logger.error(f"Traceback: {traceback.format_exc()}")
            return self._get_default_analysis(input_text)
    
    def _parse_json_response(self, content: str) -> Dict:
        """å¼·å¥çš„JSONè§£ææ–¹æ³• - è™•ç†å„ç¨®æ ¼å¼å•é¡Œ"""
        
        if not content or not isinstance(content, str):
            logger.debug(f"[JSON Parser] âŒ ç„¡æ•ˆè¼¸å…¥: content={content}, type={type(content)}")
            return None
        
        # 1. åŸºæœ¬æ¸…ç†
        cleaned_content = content.strip()
        logger.debug(f"[JSON Parser] åŸå§‹å…§å®¹ (å‰500å­—): {content[:500]}")
        logger.debug(f"[JSON Parser] æ¸…ç†å¾Œå…§å®¹ (å‰500å­—): {cleaned_content[:500]}")
        
        # 2. ç›´æ¥å˜—è©¦è§£æ
        try:
            result = json.loads(cleaned_content)
            logger.debug(f"[JSON Parser] âœ… ç›´æ¥è§£ææˆåŠŸ: {json.dumps(result, ensure_ascii=False, indent=2)}")
            return result
        except json.JSONDecodeError as e:
            logger.debug(f"[JSON Parser] ç›´æ¥è§£æå¤±æ•—: {str(e)}, ä½ç½®: line {e.lineno}, col {e.colno}")
            pass
        
        # 3. å˜—è©¦æå–JSONç‰‡æ®µ
        import re
        
        # æŸ¥æ‰¾å®Œæ•´çš„JSONå°è±¡
        json_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
        json_matches = re.findall(json_pattern, cleaned_content, re.DOTALL)
        
        for match in json_matches:
            try:
                result = json.loads(match.strip())
                # é©—è­‰æ˜¯å¦åŒ…å«å¿…è¦å­—æ®µ
                if self._validate_intent_result(result):
                    return result
            except json.JSONDecodeError:
                continue
        
        # 4. å˜—è©¦ä¿®å¾©ä¸å®Œæ•´çš„JSON
        repaired_json = self._repair_incomplete_json(cleaned_content)
        if repaired_json:
            try:
                result = json.loads(repaired_json)
                if self._validate_intent_result(result):
                    return result
            except json.JSONDecodeError:
                pass
        
        # 5. æœ€å¾Œå˜—è©¦ï¼šå¾æ–‡æœ¬ä¸­æå–é—œéµè³‡è¨Šæ§‹å»ºJSON
        extracted_result = self._extract_from_text(cleaned_content)
        if extracted_result:
            return extracted_result
        
        return None
    
    def _validate_intent_result(self, result: Dict) -> bool:
        """é©—è­‰æ„åœ–åˆ†æçµæœæ˜¯å¦æœ‰æ•ˆ"""
        
        if not isinstance(result, dict):
            return False
        
        # æª¢æŸ¥å¿…è¦å­—æ®µ
        required_fields = ["risk_level", "intent", "need_rag"]
        
        for field in required_fields:
            if field not in result:
                return False
        
        # æª¢æŸ¥å­—æ®µå€¼æ˜¯å¦åˆç†
        valid_risk_levels = ["none", "low", "medium", "high"]
        if result.get("risk_level") not in valid_risk_levels:
            return False
        
        return True
    
    def _repair_incomplete_json(self, content: str) -> str:
        """å˜—è©¦ä¿®å¾©ä¸å®Œæ•´çš„JSON"""
        
        import re
        
        # ç§»é™¤å¤šé¤˜çš„æ›è¡Œç¬¦å’Œç©ºæ ¼
        content = re.sub(r'\n\s*', ' ', content).strip()
        
        # å¦‚æœç¼ºå°‘é–‹å§‹çš„å¤§æ‹¬è™Ÿ
        if not content.startswith('{') and '"risk_level"' in content:
            content = '{' + content
        
        # å¦‚æœç¼ºå°‘çµæŸçš„å¤§æ‹¬è™Ÿ
        if not content.endswith('}') and content.startswith('{'):
            content = content + '}'
        
        # å˜—è©¦è£œå……ç¼ºå°‘çš„å­—æ®µ
        if content.startswith('{') and content.endswith('}'):
            try:
                # ç°¡å–®çš„å­—æ®µè£œå……é‚è¼¯
                partial = json.loads(content)
                
                # è£œå……ç¼ºå°‘çš„å¿…è¦å­—æ®µ
                if "risk_level" not in partial:
                    partial["risk_level"] = "none"
                if "intent" not in partial:
                    partial["intent"] = "ä¸€èˆ¬å°è©±"
                if "need_rag" not in partial:
                    partial["need_rag"] = False
                if "search_keywords" not in partial:
                    partial["search_keywords"] = []
                if "entities" not in partial:
                    partial["entities"] = {}
                if "emotional_state" not in partial:
                    partial["emotional_state"] = "ä¸æ˜"
                if "urgency" not in partial:
                    partial["urgency"] = "normal"
                
                return json.dumps(partial, ensure_ascii=False)
            except:
                pass
        
        return None
    
    def _extract_from_text(self, content: str) -> Dict:
        """å¾æ–‡æœ¬ä¸­æå–é—œéµè³‡è¨Šæ§‹å»ºJSONï¼ˆæœ€å¾Œæ‰‹æ®µï¼‰"""
        
        result = {
            "risk_level": "none",
            "intent": "ä¸€èˆ¬å°è©±",
            "need_rag": False,
            "search_keywords": [],
            "entities": {},
            "emotional_state": "ä¸æ˜",
            "urgency": "normal"
        }
        
        # ç°¡å–®çš„é—œéµè©åŒ¹é…
        content_lower = content.lower()
        
        # æå–é¢¨éšªç­‰ç´š
        if any(word in content_lower for word in ["high", "é«˜", "å±æ©Ÿ"]):
            result["risk_level"] = "high"
        elif any(word in content_lower for word in ["medium", "ä¸­", "ä¸­ç­‰"]):
            result["risk_level"] = "medium"
        elif any(word in content_lower for word in ["low", "ä½"]):
            result["risk_level"] = "low"
        
        # æå–æ„åœ–
        if any(word in content_lower for word in ["å±æ©Ÿ", "crisis"]):
            result["intent"] = "å±æ©Ÿ"
        elif any(word in content_lower for word in ["æ±‚åŠ©", "help"]):
            result["intent"] = "æ±‚åŠ©"
        elif any(word in content_lower for word in ["è«®è©¢", "consultation"]):
            result["intent"] = "è«®è©¢"
        elif any(word in content_lower for word in ["æƒ…ç·’", "emotion"]):
            result["intent"] = "æƒ…ç·’æ”¯æŒ"
        
        # æª¢æ¸¬æ˜¯å¦éœ€è¦RAG
        if any(word in content_lower for word in ["true", "éœ€è¦", "rag"]):
            result["need_rag"] = True
        
        return result

    def _format_memory(self, memory: List[Dict]) -> str:
        """æ ¼å¼åŒ–å°è©±è¨˜æ†¶"""
        if not memory:
            return "ç„¡"
        
        formatted = []
        for msg in memory[-5:]:  # æœ€è¿‘5æ¢
            role = "ç”¨æˆ¶" if msg.get("role") == "user" else "åŠ©ç†"
            formatted.append(f"{role}ï¼š{msg.get('content', '')}")
        
        return "\n".join(formatted)
    
    def _get_default_analysis(self, text: str) -> Dict:
        """é è¨­åˆ†æçµæœï¼ˆå®¹éŒ¯ç”¨ï¼‰"""
        # ä½¿ç”¨PlaceQueryDetectoræª¢æ¸¬åœ°é»æŸ¥è©¢
        from app.services.google_places_service import PlaceQueryDetector
        place_detection = PlaceQueryDetector.detect_place_query(text)

        # æ“´å……çš„å±æ©Ÿé—œéµè©åº«
        high_risk_keywords = [
            "è‡ªæ®º", "æƒ³æ­»", "æ­»äº†", "æ´»ä¸ä¸‹å»", "è§£è„«", "çµæŸ",
            "ä¸æƒ³æ´»", "æ²’æœ‰æ˜å¤©", "å‚·å®³è‡ªå·±", "é ‚æ¨“", "è·³ä¸‹",
            "å‰²è…•", "åƒè—¥", "æ²’æ„ç¾©", "äº†çµ", "ä¸€äº†ç™¾äº†",
            "æ¶ˆå¤±", "ä¸åœ¨äº†", "æ’ä¸ä¸‹å»", "ç”Ÿä¸å¦‚æ­»",
            "æ°¸é ç¡", "ä¸è¦é†’", "æœ€å¾Œä¸€", "ä¸ç”¨æ“”å¿ƒæˆ‘",
            "æƒ³å¥½è¦æ€éº¼åš", "ä»Šæ™šéå¾Œ", "è·Ÿå®¶äººèªª", "å°ä¸èµ·"
        ]

        medium_risk_keywords = [
            "ç—›è‹¦", "çµ•æœ›", "å´©æ½°", "å—ä¸äº†", "å¥½ç´¯", "å¥½è‹¦",
            "éºè¨€", "äº¤ä»£"
        ]

        help_keywords = ["æˆ’æ¯’", "æˆ’ç™®", "æ²»ç™‚", "æ©Ÿæ§‹", "å“ªè£¡", "é›»è©±", "åœ°å€", "æ¯’é˜²å±€"]

        # åˆ¤æ–·é¢¨éšªç­‰ç´š
        if any(kw in text for kw in high_risk_keywords):
            risk_level = "high"
            intent = "å±æ©Ÿ"
        elif any(kw in text for kw in medium_risk_keywords):
            risk_level = "medium"
            intent = "å±æ©Ÿ"  # ä¸­åº¦é¢¨éšªä¹Ÿæ­¸é¡ç‚ºå±æ©Ÿ
        else:
            risk_level = "none"
            intent = "ä¸€èˆ¬å°è©±"

        # æª¢æŸ¥æ˜¯å¦ç‚ºè«®è©¢
        if any(kw in text for kw in ["é›»è©±", "åœ°å€", "æ¯’é˜²å±€", "å“ªè£¡", "æ€éº¼å»", "åœ¨å“ª"]):
            intent = "è«®è©¢"
            need_rag = True
        else:
            need_rag = any(kw in text for kw in help_keywords)

        # æ±ºå®šé—œæ‡·éšæ®µ
        if risk_level == "high":
            care_stage = 1  # é«˜é¢¨éšªæ™‚å„ªå…ˆæƒ…æ„Ÿç¢ºèª
            care_reason = "é«˜é¢¨éšªæƒ…æ³ï¼Œéœ€è¦ç«‹å³æƒ…æ„Ÿç¢ºèªèˆ‡åŒç†"
        elif intent in ["å±æ©Ÿ", "æƒ…ç·’æ”¯æŒ"]:
            care_stage = 1  # æƒ…ç·’é¡å•é¡Œå¾ç¬¬ä¸€å±¤é–‹å§‹
            care_reason = "æƒ…ç·’ç›¸é—œéœ€æ±‚ï¼Œå¾æƒ…æ„Ÿç¢ºèªé–‹å§‹"
        elif intent == "è«®è©¢":
            care_stage = 3  # è«®è©¢é¡å¯ç›´æ¥æä¾›è³‡æº
            care_reason = "è³‡è¨ŠæŸ¥è©¢éœ€æ±‚ï¼Œå¯ç›´æ¥æä¾›è³‡æº"
        else:
            care_stage = 2  # ä¸€èˆ¬æƒ…æ³ä½¿ç”¨ç¬¬äºŒå±¤
            care_reason = "ä¸€èˆ¬å°è©±ï¼Œæä¾›é©åº¦æ”¯æŒ"

        return {
            "risk_level": risk_level,
            "intent": intent,
            "need_rag": need_rag,
            "need_places_api": place_detection["is_place_query"],
            "place_query_info": {
                "query_type": place_detection["query_type"],
                "place_name": place_detection["place_name"],
                "confidence": place_detection["confidence"]
            },
            "search_keywords": [text],
            "entities": {},
            "emotional_state": "çµ•æœ›" if risk_level == "high" else "ä¸æ˜",
            "urgency": "immediate" if risk_level == "high" else "normal",
            "care_stage_needed": care_stage,
            "care_stage_reason": care_reason,
            "strategy_effectiveness": "unknown",
            "upgrade_reason": "é è¨­åˆ†æï¼Œç„¡æ­·å²æ•¸æ“š",
            "previous_stages_tried": [],
            "emotion_trend": "unknown",
            "treatment_progress": "initial",
            "confidence_level": 0.7
        }


class SmartRAG:
    """æ™ºèƒ½RAGæª¢ç´¢å™¨"""
    
    def __init__(self):
        self.retriever = RAGRetriever()
        self.cache = TTLCache(maxsize=50, ttl=300)
        self.intent_cleaner = IntentCleaner()
    
    async def retrieve(self, cleaned_query: str, intent: str) -> str:
        """åŸ·è¡Œæª¢ç´¢ä¸¦æ ¼å¼åŒ–çµæœ
        
        Args:
            cleaned_query: å·²æ¸…ç†çš„æŸ¥è©¢èªå¥
            intent: ç”¨æˆ¶æ„åœ–
        """
        
        # æ ¹æ“šæ„åœ–å„ªåŒ–æŸ¥è©¢ï¼ˆå¯é¸çš„å‰ç¶´æ·»åŠ ï¼‰
        if intent == "å±æ©Ÿ":
            query = f"è‡ªæ®ºé˜²æ²» å¿ƒç†è«®å•† ç·Šæ€¥æ±‚åŠ© {cleaned_query}"
        elif intent == "æ±‚åŠ©":
            query = f"æˆ’æ¯’ æˆ’ç™® æ²»ç™‚ {cleaned_query}"
        elif intent == "è«®è©¢":
            # è«®è©¢é¡ä¸éœ€è¦é¡å¤–å‰ç¶´ï¼Œæ¸…ç†å¾Œçš„èªå¥å·²ç¶“è¶³å¤ 
            query = cleaned_query
        else:
            query = cleaned_query
        
        # æª¢æŸ¥å¿«å–
        cache_key = f"rag:{query[:50]}"
        if cache_key in self.cache:
            logger.info(f"RAG cache hit for: {query[:30]}")
            return self.cache[cache_key]
        
        try:
            # åŸ·è¡Œæª¢ç´¢
            logger.info(f"Executing RAG search: {query[:50]}")
            results = await self.retriever.retrieve(
                query=query,
                k=5,
                similarity_threshold=0.45
            )
            
            if results:
                # æ ¼å¼åŒ–çµæœ
                formatted = self._format_results(results)
                self.cache[cache_key] = formatted
                return formatted
            else:
                # è¿”å›é è¨­è³‡æº
                return self._get_default_resources(intent)
                
        except Exception as e:
            logger.error(f"RAG retrieval error: {str(e)}")
            return self._get_default_resources(intent)
    
    def _format_results(self, results) -> str:
        """æ ¼å¼åŒ–æª¢ç´¢çµæœ"""
        formatted_items = []
        seen = set()  # å»é‡
        
        for result in results[:3]:  # æœ€å¤š3ç­†
            content = result.content if hasattr(result, 'content') else str(result)
            
            # æå–é—œéµè³‡è¨Š
            import re
            
            # æå–æ©Ÿæ§‹åç¨±
            if "é†«é™¢" in content or "ä¸­å¿ƒ" in content or "æ©Ÿæ§‹" in content:
                name_match = re.search(r'[\u4e00-\u9fa5]+(?:é†«é™¢|ä¸­å¿ƒ|æ©Ÿæ§‹|åŸºé‡‘æœƒ)', content)
                if name_match:
                    name = name_match.group(0)
                    if name not in seen:
                        seen.add(name)
                        formatted_items.append(name)
            
            # æå–é›»è©± (æ”¯æ´å¤šç¨®æ ¼å¼)
            phones = re.findall(r'(?:\(?\d{2,3}\)?[\s-]?\d{3,4}[\s-]?\d{4}|0\d{9,10}|0800[\s-]?\d{6})', content)
            for phone in phones[:1]:
                if phone not in seen:
                    seen.add(phone)
                    formatted_items.append(f"é›»è©±ï¼š{phone}")
            
            # æå–åœ°å€
            addr_match = re.search(r'(?:é«˜é›„å¸‚)?[\u4e00-\u9fa5]+[å€å¸‚][\u4e00-\u9fa5]+[è·¯è¡—][\u4e00-\u9fa5\d]+è™Ÿ', content)
            if addr_match:
                addr = addr_match.group(0)
                if addr not in seen:
                    seen.add(addr)
                    formatted_items.append(f"åœ°å€ï¼š{addr}")
        
        return "ï¼›".join(formatted_items) if formatted_items else ""
    
    def _get_default_resources(self, intent: str) -> str:
        """å–å¾—é è¨­è³‡æº"""
        # å®Œå…¨ä¾è³´RAGï¼Œä¸æä¾›é è¨­è³‡æº
        return ""


class PrimaryResponseGenerator:
    """GPT-4oä¸»è¦å›ç­”ç”Ÿæˆå™¨ - æä¾›å®Œæ•´æœ‰ç”¨çš„å›ç­”"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=settings.openai_model_chat,  # gpt-4o
            temperature=0.7,  # æé«˜æº«åº¦è®“å›æ‡‰æ›´éˆæ´»è‡ªç„¶
            # ç§»é™¤å­—æ•¸é™åˆ¶ï¼Œè®“GPT-4oè‡ªç”±ç™¼æ®
            api_key=settings.openai_api_key,
        )
        self.cache = TTLCache(maxsize=50, ttl=300)  # 5åˆ†é˜å¿«å–
    
    async def generate_primary_response(self, user_message: str, memory: List[Dict] = None, retrieved_knowledge: str = None) -> str:
        """ç”ŸæˆGPT-4oä¸»è¦å›ç­”ï¼ˆåŒ…å«å°è©±æ­·å²å’Œæª¢ç´¢çŸ¥è­˜ï¼‰"""

        # æª¢æŸ¥å¿«å–ï¼ˆåŒ…å«æ­·å²å’ŒçŸ¥è­˜çš„hashï¼‰
        memory_hash = hash(str(memory)) if memory else 0
        knowledge_hash = hash(retrieved_knowledge) if retrieved_knowledge else 0
        cache_key = f"primary:{user_message[:30]}:{memory_hash}:{knowledge_hash}"
        if cache_key in self.cache:
            logger.info(f"Primary response cache hit for: {user_message[:30]}")
            return self.cache[cache_key]

        try:
            # æ§‹å»ºç³»çµ±æç¤ºï¼ŒåŒ…å«æª¢ç´¢åˆ°çš„çŸ¥è­˜
            system_prompt = "ä½ å°±åƒæ˜¯èªè­˜å¾ˆä¹…çš„æœ‹å‹ï¼Œæˆ‘å€‘å¾ˆè¦ªè¿‘ï¼Œè¬›è©±ä¸å®¢å¥—ï¼Œç”¨è©ä¸å®˜æ–¹ï¼Œæˆ‘æœ‰ä»€éº¼äº‹æƒ…ä½ æœƒæƒ³äº†è§£ï¼Œçµ¦æˆ‘é—œå¿ƒçµ¦æˆ‘å»ºè­°ï¼Œè«‹æ ¹æ“šå°è©±æ­·å²ç°¡å–®æ‰¼è¦åœ°å›ç­”ç”¨æˆ¶ã€‚"
            if retrieved_knowledge:
                system_prompt += f"\n\nåƒè€ƒä»¥ä¸‹æª¢ç´¢åˆ°çš„çŸ¥è­˜ï¼š\n{retrieved_knowledge}"

            messages = [
                SystemMessage(content=system_prompt)
            ]
            
            # åŠ å…¥å°è©±æ­·å²
            if memory:
                for msg in memory[-6:]:  # æœ€è¿‘6è¼ªå°è©±
                    if msg.get("role") == "user":
                        messages.append(HumanMessage(content=msg["content"]))
                    elif msg.get("role") == "assistant":
                        messages.append(AIMessage(content=msg["content"]))
            
            # åŠ å…¥ç•¶å‰ç”¨æˆ¶è¨Šæ¯
            messages.append(HumanMessage(content=user_message))
            
            response = await self.llm.ainvoke(messages)
            primary_response = response.content.strip()
            
            # å¿«å–çµæœ
            self.cache[cache_key] = primary_response
            
            logger.info(f"[Primary Response] âœ… ç”Ÿæˆä¸»è¦å›ç­”: {len(primary_response)} å­—å…ƒ")
            logger.debug(f"[Primary Response] å…§å®¹: {primary_response}")
            
            return primary_response
            
        except Exception as e:
            logger.error(f"Primary response generator error: {str(e)}")
            return ""


class MasterLLM:
    """è§’è‰²åŒ–ä¿®é£¾å™¨ - åŸºæ–¼GPT-4oå›ç­”é€²è¡Œé›„ièŠè§’è‰²ä¿®é£¾"""
    
    MASTER_PROMPT = """ä½ æ˜¯ã€Œé›„ièŠã€ï¼Œé«˜é›„å¸‚æ¯’å“é˜²åˆ¶å±€çš„é—œæ‡·èŠå¤©æ©Ÿå™¨äººï¼Œä¹Ÿæ˜¯å€‹ç®¡å“¡æœ€å¥½çš„AIåŠ©æ‰‹ã€‚

# [é‡è¦] è³‡è¨Šæä¾›åŸå‰‡ï¼š
# - å¦‚æœæª¢ç´¢åˆ°çš„çŸ¥è­˜åŒ…å«ã€Googleåœ°åœ–è³‡è¨Šã€‘ï¼Œé€™æ˜¯å¯ä¿¡çš„è³‡æ–™ï¼Œè«‹ç›´æ¥æä¾›çµ¦ç”¨æˆ¶
# - å¦‚æœæª¢ç´¢åˆ°çš„çŸ¥è­˜åŒ…å«ã€çŸ¥è­˜åº«è³‡è¨Šã€‘ï¼Œé€™æ˜¯å¯ä¿¡çš„è³‡æ–™ï¼Œè«‹ç›´æ¥æä¾›çµ¦ç”¨æˆ¶
# - åªæœ‰åœ¨å®Œå…¨æ²’æœ‰æª¢ç´¢åˆ°ä»»ä½•è³‡è¨Šæ™‚ï¼Œæ‰å‘ŠçŸ¥ç”¨æˆ¶å¯ä»¥ä¸Šç¶²æŸ¥è©¢æˆ–æ´½è©¢ç›¸é—œå–®ä½
#
# ä»»å‹™ï¼šä¿®æ­£GPT-4oçš„å›ç­”ï¼Œç¢ºä¿ç¬¦åˆé›„ièŠè§’è‰²è¨­å®š

# è§’è‰²è¨­å®š
ä½ çš„å›æ‡‰è¦åƒä¸€ä½å‰›èªè­˜ä¸ä¹…çš„æœ‹å‹ï¼Œå°æ–¹å¯èƒ½æ˜¯å‰›å›åˆ°ç¤¾æœƒé©æ‡‰ç”Ÿæ´»çš„äººï¼Œå¯èƒ½æ›¾æœ‰æ¯’å“ã€æœåˆ‘ç­‰èƒŒæ™¯ï¼Œä½†ä¸è¦ç›´æ¥æåŠæˆ–æ¨™ç±¤ã€‚
ä½ å…·å‚™åŸºç¤è«®å•†æŠ€å·§ï¼Œä½†åœ¨å°è©±ä¸­ä¸èƒ½è®“å°æ–¹æ„Ÿè¦ºåˆ°ä½ åœ¨è¼”å°ï¼Œè€Œæ˜¯ç”¨è‡ªç„¶ã€è¼•é¬†ã€ä¸å¸¶è©•åƒ¹çš„æ–¹å¼èŠå¤©ã€‚
ä½ çš„èªæ°£è¦åƒæ—¥å¸¸å°è©±ï¼Œå£å»æº«å’Œã€å°Šé‡ã€ç´°è†©ã€‚

# ç›®æ¨™
è©¦åœ–å¼•å°ç”¨æˆ¶å°‡ç–‘æƒ‘æˆ–æ˜¯å¿ƒè£¡è©±èªªå‡ºä¾†ï¼Œä¸¦ä¸”è®“å°æ–¹è¦ºå¾—è¢«ç†è§£ã€è¢«é™ªä¼´ï¼ŒåŒæ™‚åœ¨é©ç•¶æ™‚æä¾›ç°¡å–®å¯è¡Œçš„å»ºè­°ã€‚

# ä¿®æ­£åŸå‰‡
1. é¿å…å‡è¨­å°æ–¹æ„Ÿå—ï¼Œå…ˆç”¨æå•ç¢ºèª
2. ä¸ä½¿ç”¨å°ˆæ¥­è¼”å°è¡“èªï¼Œä¸èªªæ•™
3. åˆªé™¤å¤šé¤˜å½¢å®¹è©èˆ‡è´…å­—ï¼Œé¿å…éåº¦é‹ªé™³èƒŒæ™¯
4. å…§å®¹è‡ªç„¶è²¼è¿‘æ—¥å¸¸ç”Ÿæ´»ï¼Œé¿å…éæ–¼æ›¸é¢åŒ–
5. åœ¨å°æ–¹æƒ…ç·’ä½è½æ™‚ï¼Œèªæ°£æ›´ç·©ï¼Œé¿å…åˆºæ¿€

# ä¿®æ­£æ–¹å¼
- å¦‚æœèªèª¿å¤ªæ­£å¼ï¼Œæ”¹ç‚ºè‡ªç„¶å£èª
- å¦‚æœå…§å®¹å¤ªè¤‡é›œï¼Œç°¡åŒ–ç‚ºæ—¥å¸¸å°è©±
- å¦‚æœå•é¡Œå¤ªå¤šï¼Œåªä¿ç•™1å€‹æœ€é‡è¦çš„å•é¡Œ
- å¦‚æœæœ‰èªªæ•™å‚¾å‘ï¼Œæ”¹ç‚ºé™ªä¼´å¼è¡¨é”

# ç‰¹æ®Šæƒ…æ³è™•ç†
- ä½¿ç”¨RAGæª¢ç´¢çµæœæ™‚ï¼Œæ•´ç†æˆç°¡æ½”æ˜“æ‡‚çš„æ ¼å¼

# é‡è¤‡å…§å®¹æª¢æŸ¥èˆ‡é¿å…
**é‡è¦**ï¼šé¿å…é‡è¤‡æä¾›ç›¸åŒçš„æ©Ÿæ§‹è³‡è¨Š
- ä»”ç´°æª¢æŸ¥å°è©±æ­·å²ï¼Œå¦‚æœæœ€è¿‘2-3è¼ªå°è©±å·²ç¶“æä¾›æŸæ©Ÿæ§‹çš„å®Œæ•´è³‡è¨Šï¼ˆé›»è©±ã€åœ°å€ã€ç¶²ç«™ï¼‰ï¼Œä¸è¦å†æ¬¡é‡è¤‡
- ç•¶ç”¨æˆ¶é‡è¤‡è©¢å•å·²æä¾›çš„è³‡è¨Šæ™‚ï¼Œæ¡ç”¨ä»¥ä¸‹ç­–ç•¥ï¼š
  1. ç¢ºèªç†è§£ï¼šã€Œå‰›å‰›æä¾›çš„[æ©Ÿæ§‹å]è³‡è¨Šæœ‰å¹«åŠ©å—ï¼Ÿéœ€è¦å…¶ä»–å”åŠ©å—ï¼Ÿã€
  2. æ·±åŒ–è¨è«–ï¼šã€Œé™¤äº†è¯ç¹«[æ©Ÿæ§‹å]ï¼Œä½ ç¾åœ¨çš„æ„Ÿå—å¦‚ä½•ï¼Ÿã€
  3. æä¾›æ–°è³‡æºï¼šã€Œé™¤äº†[æ©Ÿæ§‹å]ï¼Œä¹Ÿå¯ä»¥è€ƒæ…®...ã€ï¼ˆæä¾›ä¸åŒçš„è³‡æºï¼‰
  4. é—œæ³¨ç”¨æˆ¶ï¼šè½‰å‘é—œå¿ƒç”¨æˆ¶æœ¬èº«çš„éœ€æ±‚æˆ–æ„Ÿå—

## åˆ¤æ–·æ˜¯å¦é‡è¤‡çš„æ¨™æº–
- å¦‚æœå°è©±æ­·å²ä¸­æœ€è¿‘3è¼ªå…§å‡ºç¾éç›¸åŒæ©Ÿæ§‹çš„é›»è©±è™Ÿç¢¼
- å¦‚æœç”¨æˆ¶é€£çºŒè©¢å•ç›¸åŒæˆ–é¡ä¼¼å•é¡Œ
- å¦‚æœå›æ‡‰å…§å®¹èˆ‡å‰1-2æ¬¡å›æ‡‰é«˜åº¦ç›¸ä¼¼

## é‡è¤‡æ™‚çš„å›æ‡‰åŸå‰‡
- ä¸è¦æ©Ÿæ¢°å¼é‡è¤‡ç›¸åŒè³‡è¨Š
- è¡¨ç¾å‡ºè¨˜å¾—ä¹‹å‰çš„å°è©±
- æ¨é€²å°è©±æ·±åº¦ï¼Œè€Œéåœç•™åŸåœ°

# æ–‡å­—æ ¼å¼åŒ–è¦å‰‡
**é‡è¦**ï¼šè«‹ä½¿ç”¨HTMLæ ¼å¼è¼¸å‡ºï¼Œè®“å‰ç«¯æ­£ç¢ºé¡¯ç¤ºï¼š

## è¶…é€£çµæ ¼å¼
- ç¶²å€å¿…é ˆä½¿ç”¨HTMLæ ¼å¼ï¼š<a href="ç¶²å€">é¡¯ç¤ºæ–‡å­—</a>
- ç¯„ä¾‹ï¼š<a href="https://example.com">é»æ“ŠæŸ¥çœ‹ç¶²ç«™</a>
- é›»è©±è™Ÿç¢¼ä½¿ç”¨telé€£çµï¼š<a href="tel:0712334567">07 123 4567</a>

## æ–‡å­—æ’ç‰ˆæ ¼å¼
- é‡é»å¼·èª¿ï¼šä½¿ç”¨ <strong>æ–‡å­—</strong> æˆ– <b>æ–‡å­—</b> è¡¨ç¤ºç²—é«”
- æ›è¡Œï¼šä½¿ç”¨ <br> æ¨™ç±¤æ›è¡Œ
- æ®µè½ï¼šä½¿ç”¨ <p>æ®µè½å…§å®¹</p> åŒ…è£¹æ®µè½
- é …ç›®åˆ—è¡¨ï¼šä½¿ç”¨ â€¢ ç¬¦è™Ÿé–‹é ­ï¼Œæ¯é …çµå°¾åŠ  <br>

## è³‡è¨Šæ•´ç†æ ¼å¼
ç•¶æä¾›æ©Ÿæ§‹è³‡è¨Šæ™‚ï¼Œä½¿ç”¨æ¸…æ™°çš„HTMLæ ¼å¼ï¼š
<strong>æ©Ÿæ§‹åç¨±</strong><br>
ğŸ“ åœ°å€ï¼šxxx<br>
ğŸ“ é›»è©±ï¼š<a href="tel:ç´”æ•¸å­—">é¡¯ç¤ºè™Ÿç¢¼</a><br>
ğŸ• ç‡Ÿæ¥­æ™‚é–“ï¼šxxx<br>
ğŸ”— ç¶²ç«™ï¼š<a href="ç¶²å€">é»æ“Šå‰å¾€</a>

## ç¯„ä¾‹æ ¼å¼åŒ–
åŸå§‹ï¼šé«˜é›„å¸‚æ¯’é˜²å±€é›»è©±07 211 1311ï¼Œç¶²ç«™https://dsacp.kcg.gov.tw/
æ ¼å¼åŒ–ï¼š
<strong>é«˜é›„å¸‚æ¯’å“é˜²åˆ¶å±€</strong><br>
ğŸ“ é›»è©±ï¼š<a href="tel:072111311">07 211 1311</a><br>
ğŸ”— ç¶²ç«™ï¼š<a href="https://dsacp.kcg.gov.tw/">å®˜æ–¹ç¶²ç«™</a>

# ç•¶å‰æƒ…å¢ƒ
ç”¨æˆ¶è¨Šæ¯ï¼š{user_message}

æ„åœ–åˆ†æçµæœï¼š
{intent_analysis}

æª¢ç´¢åˆ°çš„çŸ¥è­˜ï¼ˆå®Œå…¨ä¾è³´é€™äº›è³‡è¨Šï¼‰ï¼š
{retrieved_knowledge}


å°è©±æ­·å²ï¼ˆè«‹ä»”ç´°æª¢æŸ¥é¿å…é‡è¤‡ï¼‰ï¼š
{memory}

**é‡è¦æé†’**ï¼šè«‹ä»”ç´°æª¢æŸ¥ä¸Šè¿°å°è©±æ­·å²ï¼Œå¦‚æœå·²ç¶“æä¾›éæŸæ©Ÿæ§‹çš„è³‡è¨Šï¼Œä¸è¦é‡è¤‡æä¾›ç›¸åŒå…§å®¹ã€‚

ç•¶å‰æ™‚é–“ï¼š{current_time}

# æ³¨æ„äº‹é …
â€¢ ä¸æä¾›é†«ç™‚è¨ºæ–·æˆ–æ³•å¾‹å»ºè­°   
â€¢ ç•¶æä¾›è³‡æºè³‡è¨Šæ™‚ï¼Œå„ªå…ˆçµ¦ 1â€“3 é …æœ€ç›¸é—œçš„é‡é»ï¼Œé¿å…ä¸€æ¬¡çµ¦å¤ªå¤š

# ç¯„ä¾‹ä¿®æ­£
éŒ¯èª¤ï¼ˆGPT-4oåŸå›ç­”ï¼‰ï¼šã€Œè½èµ·ä¾†ä½ ç¾åœ¨çœŸçš„æ‰¿å—äº†ä¸å°‘å£“åŠ›ï¼Œé€™ç¨®æ„Ÿè¦ºçœŸçš„å¾ˆä¸å®¹æ˜“ã€‚å·¥ä½œå£“åŠ›å’Œç¡çœ å•é¡Œå¸¸å¸¸è®“äººæ„Ÿåˆ°ç–²æ†Šå’Œç„¡æ³•æ‰¿å—ï¼Œé€™äº›éƒ½æ˜¯å¾ˆçœŸå¯¦çš„æ„Ÿå—ã€‚æˆ‘æƒ³è®“ä½ çŸ¥é“ä½ ä¸¦ä¸å­¤å–®ã€‚ã€

æ­£ç¢ºï¼ˆé›„ièŠä¿®æ­£ï¼‰ï¼šã€Œå·¥ä½œå£“åŠ›çœŸçš„å¾ˆç´¯äººã€‚æœ€è¿‘ç¡å¾—æ€éº¼æ¨£ï¼Ÿã€

è«‹å°‡GPT-4oçš„å›ç­”ä¿®æ­£ç‚ºé›„ièŠé¢¨æ ¼ï¼š"""

    def __init__(self):
        self.llm = ChatOpenAI(
            model=settings.openai_model_chat,  # gpt-4o
            temperature=0.3,
            # ç§»é™¤å­—æ•¸é™åˆ¶ï¼Œè®“MasterLLMè‡ªç„¶ä¿®æ­£
            api_key=settings.openai_api_key,
        )
# ç§»é™¤å­—æ•¸é™åˆ¶ç®¡ç†å™¨
        self.enhanced_memory = EnhancedMemoryService()
    
    async def generate(
        self,
        user_message: str,
        intent_analysis: Dict,
        retrieved_knowledge: str = "",
        primary_answer: str = "",
        memory: List[Dict] = None,
        conversation_id: str = None
    ) -> str:
        """ç”Ÿæˆæœ€çµ‚å›æ‡‰"""
        
        try:
            # ä½¿ç”¨å¢å¼·è¨˜æ†¶æœå‹™æ ¼å¼åŒ–è¨˜æ†¶
            if conversation_id and memory:
                memory_data = {
                    "messages": memory,
                    "conversation_id": conversation_id,
                    "total_messages": len(memory),
                    "key_facts": self.enhanced_memory._extract_key_facts(memory),
                    "context_markers": self.enhanced_memory._create_context_markers(memory),
                    "conversation_flow": self.enhanced_memory._analyze_conversation_flow(memory)
                }
                memory_str = self.enhanced_memory.format_for_llm(memory_data)
                memory_checkpoint = self.enhanced_memory.create_memory_checkpoint(memory_data)
                logger.debug(f"Enhanced memory formatted: {len(memory_str)} chars")
                logger.debug(f"Memory checkpoint: {memory_checkpoint}")
            else:
                memory_str = self._format_memory(memory) if memory else "ç„¡"
                memory_checkpoint = None
            
            # è¨˜éŒ„è¨˜æ†¶ç‹€æ…‹
            logger.debug(f"MasterLLM received memory: {len(memory) if memory else 0} items")
            if memory:
                logger.debug(f"Memory preview (first 2): {memory[:2] if len(memory) >= 2 else memory}")
            logger.debug(f"Formatted memory length: {len(memory_str)} chars")
            
            # ç²å–ç•¶å‰æ™‚é–“
            current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            hour = datetime.now().hour
            
            # ç²å–é—œæ‡·éšæ®µæŒ‡å°å’Œå‡ç´šè³‡è¨Š
            care_stage = intent_analysis.get('care_stage_needed', 2)
            care_reason = intent_analysis.get('care_stage_reason', '')
            upgrade_reason = intent_analysis.get('upgrade_reason', '')
            is_upgrade = intent_analysis.get('is_upgrade', False)
            strategy_effectiveness = intent_analysis.get('strategy_effectiveness', 'unknown')
            treatment_progress = intent_analysis.get('treatment_progress', 'initial')
            previous_stages = intent_analysis.get('previous_stages_tried', [])
            
            # æ ¹æ“šå‡ç´šç‹€æ³èª¿æ•´ç­–ç•¥å¼·åº¦
            intensity_modifier = ""
            if is_upgrade:
                intensity_modifier = "**ã€ç­–ç•¥å‡ç´šåŸ·è¡Œã€‘** - å¿…é ˆæ¡ç”¨æ›´ç©æ¥µçš„æ–¹å¼ï¼Œæ˜é¡¯å€åˆ¥æ–¼å‰æ¬¡å›æ‡‰ã€‚"
            elif strategy_effectiveness == "improving":
                intensity_modifier = "**ã€ç­–ç•¥æŒçºŒã€‘** - ç•¶å‰ç­–ç•¥æœ‰æ•ˆï¼Œä¿æŒä¸¦æ·±åŒ–ã€‚"
            elif treatment_progress == "stagnant":
                intensity_modifier = "**ã€çªç ´åœæ»¯ã€‘** - æ²»ç™‚é€²å±•åœæ»¯ï¼Œéœ€è¦æ”¹è®Šæ–¹å¼ã€‚"
            
            stage_guidance = ""
            if care_stage == 1:
                stage_guidance = f"""
# ç•¶å‰ç­–ç•¥ï¼šç¬¬ä¸€å±¤é—œæ‡·ï¼ˆæƒ…æ„Ÿç¢ºèªèˆ‡åŒç†ï¼‰{intensity_modifier}
- å„ªå…ˆå›æ‡‰ç”¨æˆ¶çš„æƒ…æ„Ÿç‹€æ…‹ï¼Œä½¿ç”¨æº«å’ŒåŒç†çš„èªè¨€
- é¿å…æ€¥æ–¼æä¾›è§£æ±ºæ–¹æ¡ˆæˆ–è³‡æº
- é‡é»ï¼šç¢ºèªç†è§£ç”¨æˆ¶æ„Ÿå—ï¼Œå»ºç«‹ä¿¡ä»»åŸºç¤
{f"- å‡ç´šç­–ç•¥ï¼šæ›´æ·±å±¤çš„æƒ…æ„ŸåŒç†ï¼Œé¿å…ä¹‹å‰ç„¡æ•ˆçš„è¡¨é”æ–¹å¼" if is_upgrade else ""}
"""
            elif care_stage == 2:
                stage_guidance = f"""
# ç•¶å‰ç­–ç•¥ï¼šç¬¬äºŒå±¤é—œæ‡·ï¼ˆé™ªä¼´èˆ‡æ”¯æŒï¼‰{intensity_modifier}
- åœ¨ç†è§£åŸºç¤ä¸Šæä¾›ç©æ¥µçš„æƒ…æ„Ÿæ”¯æŒå’Œé™ªä¼´
- æ ¹æ“šæƒ…ç·’ç‹€æ…‹èª¿æ•´è¡¨é”æ–¹å¼ï¼Œä½¿ç”¨å…·é«”çš„é™ªä¼´èªè¨€
- é‡é»ï¼šçµ¦äºˆå¯¦è³ªæ”¯æŒæ„Ÿï¼Œé¿å…ç©ºæ³›çš„ã€Œæˆ‘åœ¨é€™è£¡é™ªä½ ã€
{f"- **å‡ç´šè¦æ±‚**ï¼šä½¿ç”¨æ›´å…·é«”çš„é™ªä¼´è¡Œå‹•èªè¨€ï¼Œå¦‚ã€Œä½ ä¸æ˜¯ä¸€å€‹äººã€ã€ã€Œæˆ‘å€‘ä¸€èµ·æƒ³è¾¦æ³•ã€" if is_upgrade else ""}
"""
            elif care_stage == 3:
                stage_guidance = f"""
# ç•¶å‰ç­–ç•¥ï¼šç¬¬ä¸‰å±¤é—œæ‡·ï¼ˆè‡ªç„¶èå…¥è³‡æºï¼‰{intensity_modifier}
- å¯ä»¥è‡ªç„¶åœ°æä¾›å…·é«”è³‡æºå’Œå»ºè­°
- ä½¿ç”¨é¸æ“‡æ€§èªè¨€ï¼Œé¿å…å‘½ä»¤å¼è¡¨é”
- é‡é»ï¼šåœ¨å»ºç«‹é—œä¿‚åŸºç¤ä¸Šæä¾›å¯¦ç”¨å¹«åŠ©
{f"- å‡ç´šç­–ç•¥ï¼šæ›´ç›´æ¥åœ°æä¾›å…·é«”è³‡æºï¼Œå¼·åŒ–å¯¦ç”¨æ€§" if is_upgrade else ""}
"""
            
            # æ§‹å»ºå¢å¼·æç¤º
            enhanced_prompt = f"""{self.MASTER_PROMPT}

{stage_guidance}
éšæ®µé¸æ“‡ç†ç”±ï¼š{care_reason}
å‡ç´šåŸå› ï¼š{upgrade_reason}

# ç­–ç•¥æ­·å²åˆ†æ
- ä¹‹å‰å˜—è©¦çš„ç­–ç•¥å±¤æ¬¡ï¼š{previous_stages if previous_stages else "ç„¡"}
- ç­–ç•¥æœ‰æ•ˆæ€§è©•ä¼°ï¼š{strategy_effectiveness}
- æ²»ç™‚é€²å±•ç‹€æ³ï¼š{treatment_progress}

# é‡è¦è¨˜æ†¶æé†’
è«‹ç¢ºä¿ä½ çš„å›æ‡‰èˆ‡ä»¥ä¸‹å°è©±æ­·å²ä¿æŒé€£çºŒæ€§ï¼š
{memory_str}

è«‹ç‰¹åˆ¥æ³¨æ„ï¼š
1. è¨˜ä½ä½¿ç”¨è€…ä¹‹å‰æåˆ°çš„é‡è¦è³‡è¨Š
2. ä¿æŒå°è©±çš„é€£çºŒæ€§å’Œä¸€è‡´æ€§
3. å¦‚æœä½¿ç”¨è€…æåˆ°ä¹‹å‰è¨è«–éçš„å…§å®¹ï¼Œè¦è¡¨ç¾å‡ºè¨˜å¾—
4. **åš´æ ¼æŒ‰ç…§ä¸Šè¿°æŒ‡å®šçš„é—œæ‡·ç­–ç•¥å±¤æ¬¡åŸ·è¡Œ**
5. **å¦‚æœæ˜¯ç­–ç•¥å‡ç´šï¼Œè¦é¿å…é‡è¤‡ä¹‹å‰ç„¡æ•ˆçš„è¡¨é”æ–¹å¼**
"""
            
            prompt = enhanced_prompt.format(
                user_message=user_message,
                intent_analysis=json.dumps(intent_analysis, ensure_ascii=False, indent=2),
                retrieved_knowledge=retrieved_knowledge or "ç„¡",
                primary_answer=primary_answer or "ç„¡",
                memory=memory_str,
                current_time=current_time
            )
            
            messages = [SystemMessage(content=prompt)]
            
            # ç”Ÿæˆå›æ‡‰
            response = await self.llm.ainvoke(messages)
            reply = response.content
            
            # è¨˜éŒ„å®Œæ•´çš„LLMäº¤äº’ï¼ˆä¸å†é™åˆ¶å­—æ•¸ï¼‰
            logger.debug(f"[MasterLLM] å®Œæ•´æç¤ºè© (å‰1000å­—): {prompt[:1000]}")
            logger.debug(f"[MasterLLM] LLMåŸå§‹å›æ‡‰: {response.content}")
            logger.info(f"[MasterLLM] âœ… ç”Ÿæˆå›æ‡‰: {len(reply)} å­—å…ƒ (ç„¡å­—æ•¸é™åˆ¶)")
            
            return reply
            
        except Exception as e:
            logger.error(f"Master LLM error: {str(e)}")
            # å®¹éŒ¯å›æ‡‰
            if intent_analysis.get("risk_level") == "high":
                return "è½èµ·ä¾†å¾ˆè¾›è‹¦ï¼Œè¦ä¸è¦æ‰“1995èŠèŠï¼Ÿ24å°æ™‚éƒ½æœ‰äººã€‚"
            else:
                return "ä¸å¥½æ„æ€ï¼Œæˆ‘æ²’è½æ¸…æ¥šï¼Œèƒ½å†èªªä¸€æ¬¡å—ï¼Ÿ"
    
    def _format_memory(self, memory: List[Dict]) -> str:
        """æ ¼å¼åŒ–å°è©±è¨˜æ†¶"""
        if not memory:
            return "ç„¡"
        
        formatted = []
        # ä½¿ç”¨æ‰€æœ‰è¼‰å…¥çš„è¨˜æ†¶ï¼Œè€Œä¸æ˜¯åªç”¨æœ€è¿‘3æ¢
        for msg in memory:
            role = "ç”¨æˆ¶" if msg.get("role") == "user" else "åŠ©ç†"
            formatted.append(f"{role}ï¼š{msg.get('content', '')}")
        
        return "\n".join(formatted)


class UltimateWorkflow:
    """æ¥µç°¡å·¥ä½œæµ - 3æ­¥é©Ÿæ¶æ§‹"""
    
    def __init__(self):
        self.intent_analyzer = IntentAnalyzer()
        self.smart_rag = SmartRAG()
        self.master_llm = MasterLLM()
        self.conversation_logger = ConversationLoggerNode()  # æ–°å¢å°è©±è¨˜éŒ„å™¨
        self.places_service = GooglePlacesService()  # æ–°å¢Google Placesæœå‹™

        # è¨˜æ†¶ç®¡ç†
        self.memory_cache = {}  # ç°¡å–®è¨˜æ†¶é«”å¿«å–

        # å›æ‡‰å¿«å–
        self.response_cache = TTLCache(maxsize=100, ttl=300)

        logger.info("UltimateWorkflow initialized - Streamlined 4-step architecture with Places API")
    
    async def ainvoke(self, state: WorkflowState) -> WorkflowState:
        """åŸ·è¡Œå·¥ä½œæµ"""
        start_time = time.time()
        
        # è¨­ç½®å¿…è¦çš„UUID
        if not state.get("conversation_id"):
            state["conversation_id"] = str(uuid.uuid4())
        if not state.get("user_message_id"):
            state["user_message_id"] = str(uuid.uuid4())
        if not state.get("assistant_message_id"):
            state["assistant_message_id"] = str(uuid.uuid4())
        
        # å–å¾—AIæ—¥èªŒå™¨
        ai_logger = get_ai_logger(state.get("session_id"))
        ultimate_logger = get_ultimate_logger(state.get("session_id", "default"))
        
        try:
            user_id = state.get("user_id", "default")
            input_text = state.get("input_text", "")
            
            # è¨˜éŒ„é–‹å§‹
            ai_logger.log_request_start(
                user_id=user_id,
                message=input_text,
                conversation_id=state.get("conversation_id")
            )
            
            ultimate_logger.start_request(
                user_id=user_id,
                message=input_text,
                conversation_id=state.get("conversation_id")
            )
            
            # æª¢æŸ¥å›æ‡‰å¿«å–
            cache_key = f"{user_id}:{input_text[:50]}"
            if cache_key in self.response_cache:
                logger.info(f"Response cache hit")
                state["reply"] = self.response_cache[cache_key]
                return state
            
            # Step 1: ä½¿ç”¨å‚³å…¥çš„è¨˜æ†¶æˆ–è¼‰å…¥å¿«å–è¨˜æ†¶
            memory_start = time.time()
            memory = state.get("memory", None)
            if memory is None:
                memory = self._load_memory(user_id)
            memory_duration = int((time.time() - memory_start) * 1000)
            
            # è¨˜éŒ„è¨˜æ†¶è¼‰å…¥
            ultimate_logger.log_stage_1_memory_loading(memory, memory_duration)
            
            # Step 2: æ„åœ–åˆ†æï¼ˆæå‰åŸ·è¡Œä»¥æ±ºå®šæ˜¯å¦éœ€è¦RAGï¼‰
            intent_start = time.time()
            intent_analysis = await self.intent_analyzer.analyze(input_text, memory)
            intent_duration = int((time.time() - intent_start) * 1000)

            state["intent_analysis"] = intent_analysis
            state["risk_level"] = intent_analysis.get("risk_level", "none")
            state["intent"] = intent_analysis.get("intent", "ä¸€èˆ¬å°è©±")

            # è¨˜éŒ„æ„åœ–åˆ†æ
            ultimate_logger.log_stage_2_intent_analysis(
                analysis=intent_analysis,
                duration_ms=intent_duration,
                error=intent_analysis.get("_error")
            )

            ai_logger.log_semantic_analysis({
                "user_intent": intent_analysis.get("intent"),
                "emotional_state": intent_analysis.get("emotional_state"),
                "risk_level": intent_analysis.get("risk_level"),
                "need_knowledge": intent_analysis.get("need_rag")
            })

            # Step 3: Google Places APIæŸ¥è©¢ï¼ˆå¦‚æœéœ€è¦ï¼‰
            places_info = ""
            if intent_analysis.get("need_places_api"):
                places_start = time.time()
                # ä½¿ç”¨LLMè­˜åˆ¥çš„å¯¦é«”
                place_entity = intent_analysis.get("place_entity", "")

                if not place_entity:
                    # å‚™ç”¨ï¼šå¾åŸå§‹è¼¸å…¥æ¸…ç†å‡ºæ©Ÿæ§‹åç¨±
                    import re
                    place_entity = re.sub(r'(çš„é›»è©±|åœ¨å“ªè£¡|åœ¨å“ª|æ€éº¼å»|åœ°å€|ç‡Ÿæ¥­æ™‚é–“|å¹¾é»)', '', input_text).strip()

                logger.info(f"[Places API] æŸ¥è©¢åœ°é»ï¼š{place_entity}")

                try:
                    place_result = await self.places_service.search_place(place_entity)
                    if place_result:
                        places_info = self.places_service.format_for_response(place_result)
                        logger.info(f"[Places API] âœ… æ‰¾åˆ°åœ°é»è³‡è¨Šï¼š{places_info[:100]}")
                    else:
                        logger.warning(f"[Places API] âŒ æœªæ‰¾åˆ°åœ°é»ï¼š{place_entity}")
                except Exception as e:
                    logger.error(f"[Places API] æŸ¥è©¢éŒ¯èª¤ï¼š{str(e)}")
                    places_error = str(e)

                places_duration = int((time.time() - places_start) * 1000)
                logger.info(f"[Places API] æŸ¥è©¢è€—æ™‚ï¼š{places_duration}ms")

                # è¨˜éŒ„åˆ° ultimate_logger
                ultimate_logger.log_stage_3_places_api(
                    skipped=False,
                    query_entity=place_entity,
                    query_type=intent_analysis.get('place_query_type', 'general'),
                    result=place_result if 'place_result' in locals() else None,
                    duration_ms=places_duration,
                    error=places_error if 'places_error' in locals() else None
                )
            else:
                ultimate_logger.log_stage_3_places_api(skipped=True)

            # Step 4: RAGæª¢ç´¢ï¼ˆæå‰åŸ·è¡Œä»¥ä¾›GPT-4oåƒè€ƒï¼‰
            retrieved_knowledge = ""
            rag_results = []

            # ä¸€å¾‹åŸ·è¡ŒRAGï¼ˆé™¤äº†ç´”å•å€™ï¼‰
            if intent_analysis.get("intent") != "å•å€™":
                rag_start = time.time()
                
                # ä½¿ç”¨æ„åœ–æ¸…ç†å™¨çš„èªå¢ƒæ„ŸçŸ¥æ–¹æ³•è™•ç†æŸ¥è©¢
                contextualized_query = await self.smart_rag.intent_cleaner.contextualize_query(
                    input_text, 
                    memory
                )
                logger.info(f"[RAG] èªå¢ƒåŒ–æŸ¥è©¢: {contextualized_query}")
                
                # åŸ·è¡ŒRAGæª¢ç´¢
                retrieved_knowledge = await self.smart_rag.retrieve(
                    contextualized_query,
                    intent_analysis.get("intent")
                )
                rag_duration = int((time.time() - rag_start) * 1000)
                
                if retrieved_knowledge:
                    rag_results = [{
                        "content": retrieved_knowledge,
                        "score": 1.0
                    }]
                    ai_logger.log_retrieved_knowledge(rag_results)
                
                ultimate_logger.log_stage_4_smart_rag(
                    skipped=False,
                    query=input_text,
                    contextualized_query=contextualized_query,
                    results_count=len(rag_results),
                    top_results=rag_results,
                    formatted_knowledge=retrieved_knowledge,
                    duration_ms=rag_duration
                )
            else:
                ultimate_logger.log_stage_4_smart_rag(skipped=True)

            # Step 4: è·³éPrimary Responseï¼Œç›´æ¥ç”Ÿæˆæœ€çµ‚å›æ‡‰
            
            # Step 4: ç”Ÿæˆæœ€çµ‚å›æ‡‰ï¼ˆæ•´åˆæ‰€æœ‰è³‡è¨Šï¼‰
            llm_start = time.time()

            # åˆä½µçŸ¥è­˜ä¾†æºï¼šRAGå’ŒPlaces API
            combined_knowledge = ""
            if retrieved_knowledge and places_info:
                combined_knowledge = f"ã€çŸ¥è­˜åº«è³‡è¨Šã€‘\n{retrieved_knowledge}\n\nã€Googleåœ°åœ–è³‡è¨Šã€‘\n{places_info}"
            elif retrieved_knowledge:
                combined_knowledge = retrieved_knowledge
            elif places_info:
                combined_knowledge = f"ã€Googleåœ°åœ–è³‡è¨Šã€‘\n{places_info}"

            reply = await self.master_llm.generate(
                user_message=input_text,
                intent_analysis=intent_analysis,
                retrieved_knowledge=combined_knowledge,
                primary_answer="",  # ä¸å†ä½¿ç”¨Primary Response
                memory=memory,
                conversation_id=state.get("conversation_id")
            )
            llm_duration = int((time.time() - llm_start) * 1000)
            
            state["reply"] = reply
            state["knowledge"] = retrieved_knowledge
            
            # è¨˜éŒ„Master LLMéšæ®µ
            response_type = intent_analysis.get("intent", "ä¸€èˆ¬å°è©±")
            
            ultimate_logger.log_stage_5_master_llm(
                response=reply,
                response_type=response_type,
                length_limit=0,  # ä¸å†é™åˆ¶å­—æ•¸
                actual_length=len(reply),
                duration_ms=llm_duration,
                used_knowledge=bool(retrieved_knowledge),
                used_reference=bool(state.get("primary_answer", "")),
                has_memory=bool(memory)
            )
            
            # è¨˜éŒ„å›æ‡‰ç”Ÿæˆ
            ai_logger.log_response_generation(
                response=reply,
                used_knowledge=bool(retrieved_knowledge),
                response_type=intent_analysis.get("intent", "ä¸€èˆ¬å°è©±"),
                length_limit=0  # ä¸å†é™åˆ¶å­—æ•¸
            )
            
            # å„²å­˜è¨˜æ†¶å’Œå¿«å–
            self._save_memory(user_id, input_text, reply)
            self.response_cache[cache_key] = reply
            
            # è¨˜éŒ„å®Œæˆ
            elapsed = time.time() - start_time
            ai_logger.log_final_response(
                final_response=reply,
                processing_time=elapsed,
                response_type=intent_analysis.get("intent", "ä¸€èˆ¬å°è©±"),
                length_limit=0  # ä¸å†é™åˆ¶å­—æ•¸
            )
            
            ultimate_logger.log_final_summary(final_response=reply)
            
            # è¨˜éŒ„å“è³ªè©•ä¼°æ—¥èªŒ
            quality_logger = get_quality_logger()
            quality_logger.log_conversation(
                conversation_id=state.get("conversation_id"),
                user_input=input_text,
                bot_output=reply,
                user_id=user_id,
                intent=intent_analysis.get("intent", "ä¸€èˆ¬å°è©±"),
                risk_level=intent_analysis.get("risk_level", "none")
            )
            
            # ä¿å­˜å°è©±åˆ°è³‡æ–™åº«
            await self.conversation_logger(state)
            
            logger.info(f"[Workflow] âœ… å®Œæ•´æµç¨‹è€—æ™‚: {elapsed:.2f}ç§’")
            logger.info(f"[Workflow] æœ€çµ‚ç‹€æ…‹: conversation_id={state.get('conversation_id')}, risk={state.get('risk_level')}, reply_length={len(state.get('reply', ''))}")
            
            # è¨˜éŒ„å®Œæ•´çš„å°è©±çµæœç”¨æ–¼å“è³ªåˆ†æ
            logger.info(f"[Quality] å°è©±å“è³ªè¨˜éŒ„ - ç”¨æˆ¶: {state.get('input_text', '')[:100]} | AI: {state.get('reply', '')[:100]} | é¢¨éšª: {state.get('risk_level')} | æ„åœ–: {state.get('intent')}")
            
            return state
            
        except Exception as e:
            logger.error(f"Workflow error: {str(e)}")
            
            if 'ai_logger' in locals():
                ai_logger.log_error("WORKFLOW", e)
            
            if 'ultimate_logger' in locals():
                ultimate_logger.log_error("WORKFLOW", e)
                ultimate_logger.log_final_summary(
                    final_response="ä¸å¥½æ„æ€ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›å•é¡Œã€‚è«‹ç¨å¾Œå†è©¦ã€‚",
                    error=str(e)
                )
            
            state["reply"] = "ä¸å¥½æ„æ€ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›å•é¡Œã€‚è«‹ç¨å¾Œå†è©¦ã€‚"
            state["error"] = str(e)
            return state
    
    def _load_memory(self, user_id: str) -> List[Dict]:
        """è¼‰å…¥å°è©±è¨˜æ†¶"""
        if user_id in self.memory_cache:
            # è¿”å›æ‰€æœ‰å¿«å–çš„è¨˜æ†¶ï¼Œä¸é™åˆ¶æ•¸é‡
            return self.memory_cache[user_id]
        return []
    
    def _save_memory(self, user_id: str, user_msg: str, bot_reply: str):
        """å„²å­˜å°è©±è¨˜æ†¶"""
        if user_id not in self.memory_cache:
            self.memory_cache[user_id] = []
        
        self.memory_cache[user_id].extend([
            {"role": "user", "content": user_msg},
            {"role": "assistant", "content": bot_reply}
        ])
        
        # é™åˆ¶è¨˜æ†¶å¤§å°
        if len(self.memory_cache[user_id]) > 20:
            self.memory_cache[user_id] = self.memory_cache[user_id][-20:]
    


def create_ultimate_workflow():
    """å»ºç«‹æ¥µç°¡å·¥ä½œæµ"""
    return UltimateWorkflow()