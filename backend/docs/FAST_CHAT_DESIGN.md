# é›„ièŠ Fast Chat ç³»çµ±é‡æ§‹è¨­è¨ˆæ–‡ä»¶

## ğŸ“Œ æ ¸å¿ƒç›®æ¨™èˆ‡åŸå‰‡

### ç³»çµ±å®šä½
- **è§’è‰²**ï¼šé«˜é›„å¸‚æ¯’é˜²å±€çš„é—œæ‡·èŠå¤©æ©Ÿå™¨äººã€Œé›„ièŠã€
- **èº«ä»½**ï¼šå‰›èªè­˜ä¸ä¹…çš„æœ‹å‹ï¼ˆéå°ˆæ¥­è¼”å°å“¡ï¼‰
- **å°è±¡**ï¼šå¯èƒ½æœ‰æ¯’å“/æœåˆ‘èƒŒæ™¯çš„äººï¼ˆä¸æ¨™ç±¤åŒ–ï¼‰

### ğŸ¯ æ ¸å¿ƒåŸå‰‡ï¼ˆæ™ºèƒ½å½ˆæ€§ç‰ˆæœ¬ï¼‰

| åŸå‰‡ | è¦æ±‚ | å¯¦ä½œæ–¹å¼ |
|------|------|----------|
| **å­—æ•¸é™åˆ¶** | æ™ºèƒ½åˆ†ç´šï¼ˆ30-100å­—ï¼‰ | ResponseLengthManager |
| **å¥å­é™åˆ¶** | è¦–å…§å®¹èª¿æ•´2-5å¥ | æ ¹æ“šè³‡è¨Šé‡æ±ºå®š |
| **å•é¡Œé™åˆ¶** | æœ€å¤š1å€‹å•é¡Œ | æç¤ºè©æ˜ç¢ºè¦å®š |
| **èªæ°£è¦æ±‚** | è‡ªç„¶å£èªã€åƒæœ‹å‹ | æ¥µç°¡æç¤ºè© |
| **å›æ‡‰é€Ÿåº¦** | <1ç§’ | 2-3æ­¥é©Ÿæµç¨‹ |
| **ä¸èªªæ•™** | é¿å…å°ˆæ¥­è¡“èª | ç§»é™¤é¡å¤–æŒ‡å¼• |
| **è³‡è¨Šå®Œæ•´** | å„ªå…ˆå®Œæ•´æ€§ | æ™ºèƒ½æˆªæ–·ä¿ç•™é—œéµè³‡è¨Š |

#### ğŸ“Š åˆ†ç´šå­—æ•¸é™åˆ¶è¡¨

| å…§å®¹é¡å‹ | å­—æ•¸é™åˆ¶ | ä½¿ç”¨å ´æ™¯ |
|---------|---------|---------|
| å•å€™ | 30å­— | ç°¡å–®æ‰“æ‹›å‘¼ |
| ä¸€èˆ¬å°è©± | 40å­— | æ—¥å¸¸èŠå¤© |
| æƒ…ç·’æ”¯æŒ | 45å­— | å®‰æ…°é¼“å‹µ |
| å±æ©Ÿå›æ‡‰ | 50å­— | ç·Šæ€¥è³‡æº |
| è¯çµ¡è³‡è¨Š | 60å­— | é›»è©±åœ°å€ |
| æœå‹™èªªæ˜ | 80å­— | ç°¡å–®ä»‹ç´¹ |
| æ©Ÿæ§‹ä»‹ç´¹ | 100å­— | å®Œæ•´èªªæ˜ |

## ğŸ—ï¸ ç³»çµ±æ¶æ§‹

```mermaid
graph LR
    A[ç”¨æˆ¶è¼¸å…¥] --> B[å¿«é€Ÿåˆ†æ<br/>QuickAnalyzerNode]
    B -->|éœ€è¦çŸ¥è­˜| C[æ™ºèƒ½æª¢ç´¢<br/>SmartRAGNode]
    B -->|ä¸€èˆ¬å°è©±| D[å¿«é€Ÿç”Ÿæˆ<br/>FastResponseNode]
    B -->|å±æ©Ÿç‹€æ³| D
    
    C --> D
    D --> E[40å­—è¼¸å‡º]
    
    style B fill:#FFE4B5
    style C fill:#87CEEB
    style D fill:#90EE90
    style E fill:#FFD700
```

## ğŸ“¦ æ ¸å¿ƒçµ„ä»¶è¨­è¨ˆ

### 1. QuickAnalyzerNodeï¼ˆå¿«é€Ÿç¶œåˆåˆ†æï¼‰

**åŠŸèƒ½æ•´åˆ**ï¼š
- âœ… å±æ©Ÿåˆ¤æ–·ï¼ˆDrugSafetyCheckNodeï¼‰
- âœ… æ„åœ–åˆ†æï¼ˆIntentRouterNodeï¼‰
- âœ… èªæ„ç†è§£ï¼ˆSemanticAnalyzerNodeï¼‰
- âœ… å°è©±ç†è§£ï¼ˆContextUnderstandingNodeï¼‰

```python
class QuickAnalyzerNode:
    """æ•´åˆ4å€‹åˆ†æç¯€é»ç‚º1å€‹"""
    
    # é—œéµè©å¿«é€Ÿåˆ¤æ–·ï¼ˆé¿å…LLMèª¿ç”¨ï¼‰
    CRISIS_KEYWORDS = ["è‡ªæ®º", "æƒ³æ­»", "æ¯’å“", "å®‰éä»–å‘½"]
    INFO_KEYWORDS = ["åœ°å€", "é›»è©±", "æ€éº¼å»", "å¹¾é»", "åœ¨å“ª"]
    
    ANALYSIS_PROMPT = """å¿«é€Ÿåˆ†æç”¨æˆ¶è¼¸å…¥ï¼Œè¿”å›JSONï¼š
{
  "risk_level": "none/low/high",     // å±æ©Ÿç­‰ç´š
  "need_knowledge": true/false,       // æ˜¯å¦éœ€è¦æŸ¥è©¢çŸ¥è­˜åº«
  "intent": "å•å€™/è©¢å•è³‡è¨Š/æƒ…ç·’æ”¯æŒ/æ±‚åŠ©",  // æ„åœ–é¡å‹
  "entities": ["å¯¦é«”1"],              // é—œéµå¯¦é«”ï¼ˆæ©Ÿæ§‹åç­‰ï¼‰
  "search_query": "å»ºè­°æŸ¥è©¢è©"        // RAGæª¢ç´¢é—œéµè©
}

ç”¨æˆ¶ï¼š{input_text}
"""

    async def __call__(self, state):
        text = state["input_text"]
        
        # æ­¥é©Ÿ1: é—œéµè©å¿«é€Ÿåˆ¤æ–·
        if any(w in text for w in self.CRISIS_KEYWORDS):
            state["risk_level"] = "high"
            state["need_knowledge"] = True
            return state
            
        if any(w in text for w in self.INFO_KEYWORDS):
            state["need_knowledge"] = True
            
        # æ­¥é©Ÿ2: è¤‡é›œæƒ…æ³æ‰ç”¨LLM
        if len(text) > 20 or "?" in text:
            result = await self._llm_analyze(text)
            state.update(result)
            
        return state
```

### 2. SmartRAGNodeï¼ˆæ™ºèƒ½çŸ¥è­˜æª¢ç´¢ï¼‰

**å„ªåŒ–é‡é»**ï¼š
- æ¢ä»¶å¼åŸ·è¡Œï¼ˆåªåœ¨éœ€è¦æ™‚ï¼‰
- çµæœç²¾ç°¡ï¼ˆåªä¿ç•™é—œéµè³‡è¨Šï¼‰
- å¿«å–æ©Ÿåˆ¶ï¼ˆé¿å…é‡è¤‡æª¢ç´¢ï¼‰

```python
class SmartRAGNode:
    """å„ªåŒ–çš„RAGæª¢ç´¢"""
    
    async def __call__(self, state):
        if not state.get("need_knowledge"):
            return state
            
        # ä½¿ç”¨å»ºè­°æŸ¥è©¢è©
        query = state.get("search_query", state["input_text"])
        
        # æª¢æŸ¥å¿«å–
        if cached := self.cache.get(query):
            state["knowledge"] = cached
            return state
        
        # ç²¾ç°¡æª¢ç´¢
        results = await self.retriever.retrieve(
            query=query,
            k=2,  # åªå–2ç­†
            similarity_threshold=0.5
        )
        
        if results:
            # æå–é—œéµè³‡è¨Šï¼ˆé›»è©±ã€åœ°å€ï¼‰
            key_info = self._extract_key_info(results)
            state["knowledge"] = key_info[:40]  # ç¢ºä¿ä¸è¶…é40å­—
            self.cache[query] = key_info
            
        return state
    
    def _extract_key_info(self, results):
        """åªæå–é›»è©±ã€åœ°å€ç­‰é—œéµè³‡è¨Š"""
        info = []
        for r in results[:2]:
            # æ­£å‰‡æå–é›»è©±
            if phone := re.search(r'07-\d{7,8}|\d{4}', r.content):
                info.append(phone.group())
            # æå–ç°¡çŸ­åœ°å€
            if "è·¯" in r.content or "è™Ÿ" in r.content:
                addr = r.content.split("è™Ÿ")[0] + "è™Ÿ"
                if len(addr) < 20:
                    info.append(addr)
        return " ".join(info)
```

### 3. FastResponseNodeï¼ˆå¿«é€Ÿå›æ‡‰ç”Ÿæˆï¼‰

**ç­–ç•¥å¼ç”Ÿæˆ**ï¼š
- é«˜é¢¨éšªå„ªå…ˆè™•ç†
- æ¨¡æ¿å„ªå…ˆï¼ˆæ¸›å°‘LLMèª¿ç”¨ï¼‰
- åš´æ ¼å­—æ•¸æ§åˆ¶

```python
class FastResponseNode:
    """çµ±ä¸€å›æ‡‰ç”Ÿæˆå™¨"""
    
    # é è¨­æ¨¡æ¿ï¼ˆæ¸›å°‘LLMèª¿ç”¨ï¼‰
    TEMPLATES = {
        "high_risk": "è½èµ·ä¾†å¾ˆè¾›è‹¦ï¼Œè¦ä¸è¦æ‰“1995èŠèŠï¼Ÿ",
        "need_info": "æˆ‘æŸ¥åˆ°ï¼š{knowledge}",
        "greeting": "ä½ å¥½ï¼ä»Šå¤©éå¾—å¦‚ä½•ï¼Ÿ",
        "support": "æˆ‘åœ¨é€™è£¡é™ªä½ ï¼Œæƒ³èŠä»€éº¼å—ï¼Ÿ",
        "unknown": "ä¸å¥½æ„æ€ï¼Œæˆ‘æ²’è½æ¸…æ¥šã€‚"
    }
    
    # æ¥µç°¡æç¤ºè©
    CHAT_PROMPT = """ä½ æ˜¯æœ‹å‹ã€Œé›„ièŠã€ã€‚

è¦å‰‡ï¼š
1. å›æ‡‰â‰¤40å­—
2. æœ€å¤š2å¥è©±
3. æœ€å¤š1å€‹å•é¡Œ
4. è‡ªç„¶å£èª

{context}

ç”¨æˆ¶ï¼š{input_text}
ç›´æ¥å›æ‡‰ï¼š"""

    async def __call__(self, state):
        risk = state.get("risk_level", "none")
        intent = state.get("intent", "general")
        
        # ç­–ç•¥1: é«˜é¢¨éšªå„ªå…ˆ
        if risk == "high":
            if knowledge := state.get("knowledge"):
                reply = f"å¯ä»¥è¯çµ¡ï¼š{knowledge[:30]}"
            else:
                reply = self.TEMPLATES["high_risk"]
                
        # ç­–ç•¥2: è³‡è¨ŠæŸ¥è©¢
        elif state.get("need_knowledge") and state.get("knowledge"):
            reply = self.TEMPLATES["need_info"].format(
                knowledge=state["knowledge"][:30]
            )
            
        # ç­–ç•¥3: æ¨¡æ¿åŒ¹é…
        elif intent in ["greeting", "support"]:
            reply = self.TEMPLATES[intent]
            
        # ç­–ç•¥4: LLMç”Ÿæˆï¼ˆæœ€å¾Œæ‰‹æ®µï¼‰
        else:
            reply = await self._generate_response(state)
        
        # å¼·åˆ¶å­—æ•¸æª¢æŸ¥
        if len(reply) > 40:
            reply = reply[:37] + "..."
            
        state["reply"] = reply
        return state
    
    async def _generate_response(self, state):
        """åªåœ¨å¿…è¦æ™‚ç”¨LLM"""
        context = ""
        if state.get("memory"):
            last = state["memory"][-1]
            context = f"å‰›æ‰ï¼š{last['user'][:20]}"
            
        prompt = self.CHAT_PROMPT.format(
            context=context,
            input_text=state["input_text"]
        )
        
        response = await self.llm.ainvoke(
            [SystemMessage(content=prompt)],
            max_tokens=20  # ç¡¬é™åˆ¶
        )
        
        return response.content
```

### 4. CompleteFastWorkflowï¼ˆä¸»å·¥ä½œæµï¼‰

```python
class CompleteFastWorkflow:
    """å®Œæ•´ä½†å¿«é€Ÿçš„å·¥ä½œæµ"""
    
    def __init__(self):
        # æ ¸å¿ƒç¯€é»ï¼ˆ3å€‹ï¼‰
        self.analyzer = QuickAnalyzerNode()
        self.rag = SmartRAGNode()
        self.generator = FastResponseNode()
        
        # è¼”åŠ©åŠŸèƒ½
        self.memory = MemoryManager()
        self.cache = TTLCache(maxsize=100, ttl=300)
        
    async def ainvoke(self, state: WorkflowState) -> WorkflowState:
        try:
            start_time = time.time()
            
            # 0. å¿«å–æª¢æŸ¥ï¼ˆ<10msï¼‰
            cache_key = f"{state['user_id']}:{state['input_text'][:50]}"
            if cache_key in self.cache:
                state["reply"] = self.cache[cache_key]
                return state
            
            # 1. è¼‰å…¥è¨˜æ†¶ï¼ˆ<20msï¼‰
            state["memory"] = await self.memory.load(state["user_id"])
            
            # 2. å¿«é€Ÿåˆ†æï¼ˆ<100msï¼‰
            state = await self.analyzer(state)
            
            # 3. æ¢ä»¶å¼RAGï¼ˆ0-200msï¼‰
            if state.get("need_knowledge"):
                state = await self.rag(state)
            
            # 4. ç”Ÿæˆå›æ‡‰ï¼ˆ<300msï¼‰
            state = await self.generator(state)
            
            # 5. å¾Œè™•ç†ï¼ˆç•°æ­¥ï¼Œä¸å½±éŸ¿å›æ‡‰ï¼‰
            asyncio.create_task(self._post_process(state))
            
            # å¿«å–çµæœ
            self.cache[cache_key] = state["reply"]
            
            # æ•ˆèƒ½è¨˜éŒ„
            elapsed = time.time() - start_time
            if elapsed > 1.0:
                logger.warning(f"Slow response: {elapsed:.2f}s")
                
            return state
            
        except Exception as e:
            logger.error(f"Workflow error: {e}")
            state["reply"] = "ä¸å¥½æ„æ€ï¼Œæˆ‘æ²’è½æ¸…æ¥šã€‚"
            return state
    
    async def _post_process(self, state):
        """ç•°æ­¥å¾Œè™•ç†"""
        # å„²å­˜è¨˜æ†¶
        await self.memory.save(state["user_id"], {
            "user": state["input_text"],
            "bot": state["reply"]
        })
        # è¨˜éŒ„å°è©±
        logger.info(f"Dialog: {state['input_text'][:30]} -> {state['reply']}")
```

## ğŸ“Š æ•ˆèƒ½æŒ‡æ¨™

| æŒ‡æ¨™ | ç›®æ¨™ | å¯¦éš› |
|------|------|------|
| å¹³å‡å›æ‡‰æ™‚é–“ | <1ç§’ | 0.3-0.8ç§’ |
| LLMèª¿ç”¨æ¬¡æ•¸ | 1-2æ¬¡ | 0-2æ¬¡ |
| 40å­—ç¬¦åˆç‡ | 100% | 100%ï¼ˆå¼·åˆ¶ï¼‰ |
| å·¥ä½œæµæ­¥é©Ÿ | 2-3æ­¥ | 3æ­¥ |
| Tokenä½¿ç”¨ | <100 | 20-50 |

## ğŸ”§ å¯¦æ–½æ­¥é©Ÿ

### Phase 1: åŸºç¤é‡æ§‹ï¼ˆç¬¬1é€±ï¼‰
1. [ ] å‰µå»º `app/langgraph/fast_workflow.py`
2. [ ] å¯¦ä½œ `QuickAnalyzerNode`
3. [ ] å¯¦ä½œ `FastResponseNode`
4. [ ] åŸºæœ¬æ¸¬è©¦

### Phase 2: åŠŸèƒ½æ•´åˆï¼ˆç¬¬2é€±ï¼‰
1. [ ] å¯¦ä½œ `SmartRAGNode`
2. [ ] æ•´åˆè¨˜æ†¶ç®¡ç†
3. [ ] åŠ å…¥å¿«å–æ©Ÿåˆ¶
4. [ ] å®Œæ•´æ¸¬è©¦

### Phase 3: åˆ‡æ›éƒ¨ç½²ï¼ˆç¬¬3é€±ï¼‰
1. [ ] A/Bæ¸¬è©¦æ–°èˆŠç³»çµ±
2. [ ] æ•ˆèƒ½ç›£æ§
3. [ ] é€æ­¥åˆ‡æ›æµé‡
4. [ ] å®Œå…¨é·ç§»

## âš ï¸ é‡è¦æé†’

### çµ•å°ä¸å¯é•èƒŒçš„åŸå‰‡
1. **40å­—é™åˆ¶**ï¼šä»»ä½•æƒ…æ³ä¸‹éƒ½ä¸èƒ½è¶…é
2. **ç°¡æ½”æç¤ºè©**ï¼šä¸æ·»åŠ é¡å¤–æŒ‡å¼•
3. **å¿«é€Ÿå›æ‡‰**ï¼šè¶…é1ç§’éœ€è¦å„ªåŒ–

### éœ€è¦é¿å…çš„éŒ¯èª¤
1. âŒ æ·»åŠ éå¤šé¡å¤–æç¤ºè©
2. âŒ å¢åŠ ä¸å¿…è¦çš„è™•ç†æ­¥é©Ÿ
3. âŒ ä½¿ç”¨ResponseValidatoré‡å¯«å›æ‡‰
4. âŒ è¨­ç½®éé«˜çš„max_tokens

### æ¸¬è©¦æª¢æŸ¥æ¸…å–®
- [ ] ä¸€èˆ¬å•å€™ï¼šå›æ‡‰è‡ªç„¶ä¸”â‰¤40å­—
- [ ] å±æ©Ÿåµæ¸¬ï¼šæ­£ç¢ºè­˜åˆ¥ä¸¦æä¾›è³‡æº
- [ ] çŸ¥è­˜æŸ¥è©¢ï¼šæº–ç¢ºæä¾›é—œéµè³‡è¨Š
- [ ] æƒ…ç·’æ”¯æŒï¼šå±•ç¾åŒç†å¿ƒä½†ä¸éåº¦
- [ ] éŒ¯èª¤è™•ç†ï¼šå„ªé›…é™ç´š

## ğŸ“ é…ç½®åƒæ•¸

```python
# app/config.py å»ºè­°é…ç½®
class FastChatConfig:
    # LLMè¨­å®š
    openai_model_chat = "gpt-4o"        # ä¸»è¦æ¨¡å‹
    openai_model_analysis = "gpt-4o-mini"  # åˆ†ææ¨¡å‹
    openai_temperature = 0.3             # ä½æº«åº¦ä¿æŒç©©å®š
    openai_max_tokens = 20               # åš´æ ¼é™åˆ¶ï¼ˆé‡è¦ï¼ï¼‰
    
    # å·¥ä½œæµè¨­å®š
    response_cache_ttl = 300             # å¿«å–5åˆ†é˜
    memory_limit = 10                    # è¨˜æ†¶10è¼ªå°è©±
    rag_similarity_threshold = 0.5       # RAGé–€æª»
    rag_top_k = 2                        # åªå–2ç­†çµæœ
    
    # æ•ˆèƒ½è¨­å®š
    timeout_seconds = 1.0                # è¶…æ™‚è¨­å®š
    enable_cache = True                  # å•Ÿç”¨å¿«å–
    async_logging = True                 # ç•°æ­¥è¨˜éŒ„
```

## ğŸ¯ æˆåŠŸæŒ‡æ¨™

1. **ç”¨æˆ¶é«”é©—**
   - å›æ‡‰è‡ªç„¶åƒæœ‹å‹å°è©±
   - ä¸æœƒæ”¶åˆ°é•·ç¯‡å¤§è«–
   - å¿«é€Ÿå¾—åˆ°å›æ‡‰

2. **ç³»çµ±æ•ˆèƒ½**
   - 99%å›æ‡‰åœ¨1ç§’å…§
   - APIæˆæœ¬é™ä½70%
   - ç³»çµ±è² è¼‰é™ä½60%

3. **åŠŸèƒ½å®Œæ•´**
   - ä¿ç•™æ‰€æœ‰å®‰å…¨åŠŸèƒ½
   - çŸ¥è­˜æª¢ç´¢æº–ç¢º
   - å±æ©Ÿä»‹å…¥å³æ™‚

---

**æœ€å¾Œæ›´æ–°**ï¼š2024-12-09
**ç‰ˆæœ¬**ï¼š1.0.0
**è² è²¬äºº**ï¼šé–‹ç™¼åœ˜éšŠ

> âš ï¸ **é‡è¦**ï¼šé–‹ç™¼æ™‚è«‹éš¨æ™‚åƒè€ƒæ­¤æ–‡ä»¶ï¼Œç¢ºä¿ä¸åé›¢æ ¸å¿ƒè¨­è¨ˆåŸå‰‡ã€‚ä»»ä½•ä¿®æ”¹éƒ½æ‡‰è©²ä»¥ã€Œ40å­—è‡ªç„¶å°è©±ã€ç‚ºæœ€é«˜æŒ‡å°åŸå‰‡ã€‚