# 05 - 脫敏機制設計文件 [暫緩實作]

## ⚠️ 注意：此功能暫緩實作
根據最新決定，脫敏機制暫時不實作。此文件保留作為未來參考。

## 快速恢復指南
如果你忘記了這個模組，記住：這是資料保護的關鍵層，負責在儲存和傳輸前移除或遮罩敏感資訊。使用規則引擎 + NER 模型雙層檢測，確保 PII 不外洩。

## 核心技術棧
- spaCy / Stanza (NER 模型)
- regex (規則引擎)
- hashlib (假名化)
- Faker (資料模擬)

## 專案結構
```
redaction/
├── __init__.py
├── redactor.py         # 主要脫敏引擎
├── patterns.py         # 脫敏規則定義
├── ner_detector.py     # NER 偵測器
├── pseudonymizer.py    # 假名化處理
├── rules/              # 規則配置
│   ├── tw_rules.yaml   # 台灣規則
│   ├── medical.yaml    # 醫療敏感詞
│   └── financial.yaml  # 金融資訊
└── models/             # NER 模型
    ├── zh_tw_model/    # 繁體中文模型
    └── en_model/       # 英文模型
```

## 1. 脫敏規則定義

```python
# patterns.py
import re
from typing import Dict, List, Pattern
from dataclasses import dataclass

@dataclass
class RedactionRule:
    """脫敏規則"""
    name: str
    pattern: Pattern
    replacement: str
    severity: str  # low, medium, high
    category: str

class RedactionPatterns:
    """脫敏規則集合"""
    
    # 台灣身分證字號
    TW_ID_PATTERN = re.compile(
        r'\b[A-Z][12]\d{8}\b',
        re.IGNORECASE
    )
    
    # 手機號碼 (台灣)
    TW_MOBILE_PATTERN = re.compile(
        r'\b09\d{8}\b'
    )
    
    # 市話 (含區碼)
    TW_PHONE_PATTERN = re.compile(
        r'\b0[2-9]-?\d{7,8}\b'
    )
    
    # Email
    EMAIL_PATTERN = re.compile(
        r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    )
    
    # 信用卡號
    CREDIT_CARD_PATTERN = re.compile(
        r'\b(?:\d{4}[-\s]?){3}\d{4}\b'
    )
    
    # 銀行帳號
    BANK_ACCOUNT_PATTERN = re.compile(
        r'\b\d{10,16}\b'  # 需要配合上下文
    )
    
    # 地址 (複雜，需要 NER 輔助)
    ADDRESS_KEYWORDS = re.compile(
        r'(路|街|巷|弄|號|樓|室|段|里|鄰|區|市|縣|鄉|鎮)',
        re.IGNORECASE
    )
    
    # 姓名 (需要 NER)
    NAME_PATTERN = re.compile(
        r'\b[王李張劉陳楊黃趙周吳鄭孫朱馬羅梁宋謝唐許韓馮鄧曹彭曾蕭田董潘袁于蔣蔡余葉蘇呂魏程丁沈姚盧汪楚閻儲連季俞聶谷狄寗應]{1}[^\s,，。.!?！？]{1,3}\b'
    )
    
    # 健保卡號
    NHI_CARD_PATTERN = re.compile(
        r'\b[A-Z0-9]{10}\b'
    )
    
    # 護照號碼
    PASSPORT_PATTERN = re.compile(
        r'\b[A-Z0-9]{9}\b'
    )
    
    # 車牌號碼
    LICENSE_PLATE_PATTERN = re.compile(
        r'\b[A-Z]{2,3}-\d{4}\b'
    )
    
    # 統一編號
    COMPANY_ID_PATTERN = re.compile(
        r'\b\d{8}\b'
    )
    
    # 醫療相關敏感詞
    MEDICAL_KEYWORDS = [
        "愛滋", "HIV", "癌症", "精神疾病", "憂鬱症", 
        "思覺失調", "糖尿病", "肝炎", "結核病"
    ]
    
    # 毒品相關詞彙
    DRUG_KEYWORDS = [
        "海洛因", "安非他命", "搖頭丸", "K他命", "大麻",
        "古柯鹼", "嗎啡", "鴉片", "美沙冬", "丁基原啡因"
    ]
    
    @classmethod
    def get_all_rules(cls) -> List[RedactionRule]:
        """取得所有規則"""
        rules = [
            RedactionRule(
                name="tw_id",
                pattern=cls.TW_ID_PATTERN,
                replacement="[身分證]",
                severity="high",
                category="pii"
            ),
            RedactionRule(
                name="mobile",
                pattern=cls.TW_MOBILE_PATTERN,
                replacement="09XX-XXX-XXX",
                severity="medium",
                category="contact"
            ),
            RedactionRule(
                name="phone",
                pattern=cls.TW_PHONE_PATTERN,
                replacement="0X-XXXX-XXXX",
                severity="medium",
                category="contact"
            ),
            RedactionRule(
                name="email",
                pattern=cls.EMAIL_PATTERN,
                replacement="[email]",
                severity="medium",
                category="contact"
            ),
            RedactionRule(
                name="credit_card",
                pattern=cls.CREDIT_CARD_PATTERN,
                replacement="XXXX-XXXX-XXXX-XXXX",
                severity="high",
                category="financial"
            ),
            RedactionRule(
                name="nhi_card",
                pattern=cls.NHI_CARD_PATTERN,
                replacement="[健保卡號]",
                severity="high",
                category="medical"
            ),
            RedactionRule(
                name="passport",
                pattern=cls.PASSPORT_PATTERN,
                replacement="[護照號碼]",
                severity="high",
                category="pii"
            ),
            RedactionRule(
                name="license_plate",
                pattern=cls.LICENSE_PLATE_PATTERN,
                replacement="[車牌]",
                severity="low",
                category="pii"
            ),
            RedactionRule(
                name="company_id",
                pattern=cls.COMPANY_ID_PATTERN,
                replacement="[統編]",
                severity="low",
                category="business"
            )
        ]
        
        return rules
```

## 2. 主要脫敏引擎

```python
# redactor.py
from typing import Dict, List, Tuple, Optional
import re
import hashlib
from dataclasses import dataclass

@dataclass
class RedactionResult:
    """脫敏結果"""
    original: str
    redacted: str
    found_items: List[Dict]
    summary: str

class Redactor:
    """主要脫敏引擎"""
    
    def __init__(self, config: dict = None):
        self.config = config or {}
        self.patterns = RedactionPatterns()
        self.rules = self.patterns.get_all_rules()
        self.ner_detector = NERDetector()
        self.pseudonymizer = Pseudonymizer()
        
        # 載入自訂規則
        self._load_custom_rules()
    
    def redact(
        self, 
        text: str, 
        mode: str = "mask",  # mask, hash, fake, remove
        lang: str = "zh-TW",
        context: dict = None
    ) -> RedactionResult:
        """執行脫敏"""
        
        original = text
        found_items = []
        
        # 第一層：規則檢測
        text, rule_items = self._apply_rules(text, mode)
        found_items.extend(rule_items)
        
        # 第二層：NER 檢測
        text, ner_items = self._apply_ner(text, lang, mode)
        found_items.extend(ner_items)
        
        # 第三層：關鍵字檢測
        text, keyword_items = self._apply_keywords(text, mode)
        found_items.extend(keyword_items)
        
        # 產生摘要
        summary = self._generate_summary(original, text, len(found_items))
        
        return RedactionResult(
            original=original,
            redacted=text,
            found_items=found_items,
            summary=summary
        )
    
    def _apply_rules(self, text: str, mode: str) -> Tuple[str, List[Dict]]:
        """應用規則脫敏"""
        
        found_items = []
        
        for rule in self.rules:
            matches = list(rule.pattern.finditer(text))
            
            for match in reversed(matches):  # 反向處理避免位置偏移
                start, end = match.span()
                matched_text = match.group()
                
                # 記錄發現項目
                found_items.append({
                    "type": rule.name,
                    "category": rule.category,
                    "severity": rule.severity,
                    "position": (start, end),
                    "original": matched_text
                })
                
                # 執行替換
                replacement = self._get_replacement(
                    matched_text, 
                    rule, 
                    mode
                )
                
                text = text[:start] + replacement + text[end:]
        
        return text, found_items
    
    def _apply_ner(self, text: str, lang: str, mode: str) -> Tuple[str, List[Dict]]:
        """應用 NER 檢測"""
        
        entities = self.ner_detector.detect(text, lang)
        found_items = []
        
        # 按位置排序 (反向)
        entities.sort(key=lambda x: x["start"], reverse=True)
        
        for entity in entities:
            if entity["label"] in ["PERSON", "LOC", "ORG", "ID"]:
                start = entity["start"]
                end = entity["end"]
                matched_text = entity["text"]
                
                found_items.append({
                    "type": f"ner_{entity['label'].lower()}",
                    "category": "ner",
                    "severity": "medium",
                    "position": (start, end),
                    "original": matched_text
                })
                
                # 替換
                replacement = self._get_ner_replacement(
                    entity, 
                    mode
                )
                
                text = text[:start] + replacement + text[end:]
        
        return text, found_items
    
    def _apply_keywords(self, text: str, mode: str) -> Tuple[str, List[Dict]]:
        """應用關鍵字檢測"""
        
        found_items = []
        
        # 醫療敏感詞
        for keyword in RedactionPatterns.MEDICAL_KEYWORDS:
            if keyword in text:
                text = text.replace(keyword, "[醫療資訊]")
                found_items.append({
                    "type": "medical_keyword",
                    "category": "medical",
                    "severity": "high",
                    "original": keyword
                })
        
        # 毒品相關詞 (部分保留用於服務)
        for keyword in RedactionPatterns.DRUG_KEYWORDS:
            # 保留前兩個字元
            if len(keyword) > 2:
                replacement = keyword[:2] + "*" * (len(keyword) - 2)
                text = text.replace(keyword, replacement)
                found_items.append({
                    "type": "drug_keyword",
                    "category": "drug",
                    "severity": "medium",
                    "original": keyword
                })
        
        return text, found_items
    
    def _get_replacement(
        self, 
        matched_text: str, 
        rule: RedactionRule, 
        mode: str
    ) -> str:
        """取得替換文字"""
        
        if mode == "mask":
            return rule.replacement
        
        elif mode == "hash":
            # 使用 hash 保持一致性
            hash_val = hashlib.md5(matched_text.encode()).hexdigest()[:8]
            return f"[{rule.name}:{hash_val}]"
        
        elif mode == "fake":
            # 產生假資料
            return self.pseudonymizer.generate_fake(rule.name)
        
        elif mode == "remove":
            return ""
        
        else:
            return rule.replacement
    
    def _get_ner_replacement(self, entity: dict, mode: str) -> str:
        """取得 NER 替換文字"""
        
        label = entity["label"]
        text = entity["text"]
        
        if mode == "mask":
            if label == "PERSON":
                # 保留姓氏
                if len(text) > 1:
                    return text[0] + "○" * (len(text) - 1)
                return "○"
            elif label == "LOC":
                return "[地址]"
            elif label == "ORG":
                return "[機構]"
            else:
                return "[資訊]"
        
        elif mode == "hash":
            hash_val = hashlib.md5(text.encode()).hexdigest()[:6]
            return f"[{label}:{hash_val}]"
        
        elif mode == "fake":
            return self.pseudonymizer.generate_fake_ner(label)
        
        elif mode == "remove":
            return ""
        
        else:
            return f"[{label}]"
    
    def _generate_summary(
        self, 
        original: str, 
        redacted: str, 
        items_count: int
    ) -> str:
        """產生脫敏摘要"""
        
        if len(redacted) > 200:
            summary = redacted[:200] + "..."
        else:
            summary = redacted
        
        # 加入統計資訊
        if items_count > 0:
            summary += f"\n(已脫敏 {items_count} 項敏感資訊)"
        
        return summary
    
    def _load_custom_rules(self):
        """載入自訂規則"""
        # TODO: 從 YAML 載入自訂規則
        pass
```

## 3. NER 偵測器

```python
# ner_detector.py
import spacy
from typing import List, Dict

class NERDetector:
    """命名實體識別偵測器"""
    
    def __init__(self):
        # 載入模型
        self.models = {
            "zh-TW": self._load_chinese_model(),
            "en": self._load_english_model()
        }
    
    def _load_chinese_model(self):
        """載入中文 NER 模型"""
        try:
            # 使用 spaCy 中文模型
            import zh_core_web_sm
            nlp = zh_core_web_sm.load()
            
            # 或使用 Stanza
            # import stanza
            # nlp = stanza.Pipeline('zh-hant', processors='tokenize,ner')
            
            return nlp
        except:
            return None
    
    def _load_english_model(self):
        """載入英文 NER 模型"""
        try:
            import en_core_web_sm
            return en_core_web_sm.load()
        except:
            return None
    
    def detect(self, text: str, lang: str = "zh-TW") -> List[Dict]:
        """偵測命名實體"""
        
        nlp = self.models.get(lang)
        if not nlp:
            return []
        
        entities = []
        
        try:
            doc = nlp(text)
            
            for ent in doc.ents:
                # 對應標籤
                label_map = {
                    "PER": "PERSON",
                    "PERSON": "PERSON",
                    "LOC": "LOC",
                    "GPE": "LOC",  # 地理政治實體
                    "ORG": "ORG",
                    "NORP": "ORG",  # 國籍、宗教、政治團體
                    "FAC": "LOC",  # 設施
                    "CARDINAL": "NUMBER",
                    "MONEY": "FINANCIAL",
                    "DATE": "DATE",
                    "TIME": "TIME"
                }
                
                mapped_label = label_map.get(ent.label_, ent.label_)
                
                entities.append({
                    "text": ent.text,
                    "label": mapped_label,
                    "start": ent.start_char,
                    "end": ent.end_char,
                    "confidence": getattr(ent, "confidence", 1.0)
                })
            
            # 補充檢測 (規則輔助)
            entities.extend(self._rule_based_ner(text))
            
        except Exception as e:
            print(f"NER detection error: {e}")
        
        return entities
    
    def _rule_based_ner(self, text: str) -> List[Dict]:
        """基於規則的 NER 補充"""
        
        entities = []
        
        # 檢測包含「先生」「小姐」「女士」的姓名
        import re
        name_pattern = re.compile(
            r'([王李張劉陳楊黃趙周吳鄭孫朱馬羅梁宋謝唐許韓馮鄧曹彭曾蕭田董潘袁于蔣蔡余葉蘇呂魏程丁沈姚盧汪楚閻儲連季俞聶谷狄寗應]\S{1,2})(先生|小姐|女士)'
        )
        
        for match in name_pattern.finditer(text):
            entities.append({
                "text": match.group(1),
                "label": "PERSON",
                "start": match.start(1),
                "end": match.end(1),
                "confidence": 0.8
            })
        
        # 檢測地址
        address_pattern = re.compile(
            r'([\u4e00-\u9fa5]+(市|縣|區|鄉|鎮|路|街|段|巷|弄|號)[\u4e00-\u9fa50-9]+)'
        )
        
        for match in address_pattern.finditer(text):
            entities.append({
                "text": match.group(),
                "label": "LOC",
                "start": match.start(),
                "end": match.end(),
                "confidence": 0.7
            })
        
        return entities
```

## 4. 假名化處理

```python
# pseudonymizer.py
from faker import Faker
import hashlib
import random

class Pseudonymizer:
    """假名化處理器"""
    
    def __init__(self, seed: int = None):
        self.faker_tw = Faker('zh_TW')
        self.faker_en = Faker('en_US')
        
        if seed:
            Faker.seed(seed)
            random.seed(seed)
        
        # 一致性映射表
        self.mapping = {}
    
    def generate_fake(self, data_type: str) -> str:
        """產生假資料"""
        
        generators = {
            "tw_id": self._fake_tw_id,
            "mobile": lambda: "09" + "".join([str(random.randint(0,9)) for _ in range(8)]),
            "phone": lambda: f"0{random.randint(2,9)}-{random.randint(1000000,9999999)}",
            "email": self.faker_tw.email,
            "credit_card": self.faker_tw.credit_card_number,
            "bank_account": lambda: "".join([str(random.randint(0,9)) for _ in range(14)]),
            "nhi_card": lambda: "".join([random.choice("ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789") for _ in range(10)]),
            "passport": lambda: "".join([random.choice("ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789") for _ in range(9)]),
            "license_plate": lambda: f"{random.choice(['ABC','TPE','KHH'])}-{random.randint(1000,9999)}",
            "company_id": lambda: "".join([str(random.randint(0,9)) for _ in range(8)])
        }
        
        generator = generators.get(data_type, lambda: "[REDACTED]")
        return generator()
    
    def generate_fake_ner(self, label: str) -> str:
        """產生假的 NER 資料"""
        
        if label == "PERSON":
            return self.faker_tw.name()
        elif label == "LOC":
            return self.faker_tw.address()
        elif label == "ORG":
            return self.faker_tw.company()
        else:
            return f"[{label}]"
    
    def _fake_tw_id(self) -> str:
        """產生假的身分證字號"""
        
        # 簡化版，不做檢查碼計算
        letters = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
        letter = random.choice(letters)
        gender = random.choice(["1", "2"])
        numbers = "".join([str(random.randint(0,9)) for _ in range(8)])
        
        return f"{letter}{gender}{numbers}"
    
    def get_consistent_fake(self, original: str, data_type: str) -> str:
        """取得一致性假資料"""
        
        # 使用 hash 作為 key
        key = hashlib.md5(f"{original}:{data_type}".encode()).hexdigest()
        
        if key not in self.mapping:
            self.mapping[key] = self.generate_fake(data_type)
        
        return self.mapping[key]
```

## 5. 批次處理與效能優化

```python
# batch_processor.py
from typing import List, Dict
import asyncio
from concurrent.futures import ThreadPoolExecutor

class BatchRedactor:
    """批次脫敏處理器"""
    
    def __init__(self, redactor: Redactor, max_workers: int = 4):
        self.redactor = redactor
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
    
    async def redact_batch(
        self, 
        texts: List[str],
        mode: str = "mask",
        lang: str = "zh-TW"
    ) -> List[RedactionResult]:
        """批次脫敏"""
        
        loop = asyncio.get_event_loop()
        
        # 平行處理
        tasks = [
            loop.run_in_executor(
                self.executor,
                self.redactor.redact,
                text,
                mode,
                lang,
                None
            )
            for text in texts
        ]
        
        results = await asyncio.gather(*tasks)
        return results
    
    def redact_dataframe(self, df, columns: List[str], mode: str = "mask"):
        """處理 DataFrame"""
        
        import pandas as pd
        
        for col in columns:
            if col in df.columns:
                df[f"{col}_redacted"] = df[col].apply(
                    lambda x: self.redactor.redact(str(x), mode).redacted 
                    if pd.notna(x) else None
                )
        
        return df
```

## 6. 設定與規則管理

```yaml
# rules/tw_rules.yaml
rules:
  - name: tw_health_insurance_id
    pattern: '\b[A-Z0-9]{10}\b'
    replacement: '[健保ID]'
    severity: high
    category: medical
    
  - name: tw_military_id
    pattern: '\b[A-Z]\d{9}\b'
    replacement: '[軍人證號]'
    severity: high
    category: pii
    
  - name: drug_slang
    keywords:
      - K仔
      - 冰糖
      - 搖搖
      - 飛行員
      - 快樂丸
    replacement: '[藥物俗稱]'
    severity: medium
    category: drug

# 醫療敏感詞
medical_terms:
  high_sensitivity:
    - 愛滋病
    - HIV陽性
    - 精神分裂
    - 自殺傾向
  
  medium_sensitivity:
    - 憂鬱症
    - 焦慮症
    - 恐慌症
    
  low_sensitivity:
    - 感冒
    - 頭痛
    - 失眠

# 地區特定規則
regional:
  kaohsiung:
    landmarks:
      - 高雄市政府
      - 愛河
      - 西子灣
    replacement: '[高雄地標]'
```

## 7. 使用範例與測試

```python
# test_redaction.py
import unittest

class TestRedaction(unittest.TestCase):
    
    def setUp(self):
        self.redactor = Redactor()
    
    def test_taiwan_id(self):
        """測試身分證脫敏"""
        text = "我的身分證是A123456789"
        result = self.redactor.redact(text, mode="mask")
        
        self.assertEqual(result.redacted, "我的身分證是[身分證]")
        self.assertEqual(len(result.found_items), 1)
        self.assertEqual(result.found_items[0]["type"], "tw_id")
    
    def test_phone_number(self):
        """測試電話脫敏"""
        text = "請打0912345678或02-12345678"
        result = self.redactor.redact(text, mode="mask")
        
        self.assertIn("09XX-XXX-XXX", result.redacted)
        self.assertIn("0X-XXXX-XXXX", result.redacted)
    
    def test_medical_keywords(self):
        """測試醫療關鍵字"""
        text = "患者有憂鬱症和糖尿病病史"
        result = self.redactor.redact(text, mode="mask")
        
        self.assertIn("[醫療資訊]", result.redacted)
    
    def test_name_detection(self):
        """測試姓名偵測"""
        text = "請問王小明先生在嗎？李美華小姐已經離開了。"
        result = self.redactor.redact(text, mode="mask", lang="zh-TW")
        
        self.assertIn("王○○", result.redacted)
        self.assertIn("李○○", result.redacted)
    
    def test_address_detection(self):
        """測試地址偵測"""
        text = "送貨地址：高雄市前金區中正四路211號"
        result = self.redactor.redact(text, mode="mask")
        
        self.assertIn("[地址]", result.redacted)
    
    def test_consistency(self):
        """測試一致性假名化"""
        pseudonymizer = Pseudonymizer(seed=42)
        
        fake1 = pseudonymizer.get_consistent_fake("A123456789", "tw_id")
        fake2 = pseudonymizer.get_consistent_fake("A123456789", "tw_id")
        
        self.assertEqual(fake1, fake2)
    
    def test_performance(self):
        """測試效能"""
        import time
        
        text = "這是一段包含身分證A123456789和電話0912345678的長文字" * 100
        
        start = time.time()
        result = self.redactor.redact(text)
        duration = time.time() - start
        
        self.assertLess(duration, 1.0)  # 應該在1秒內完成

# 整合測試
def integration_test():
    """整合測試範例"""
    
    # 初始化
    redactor = Redactor()
    
    # 測試對話
    conversation = """
    使用者：我是王大明，身分證A123456789，住在高雄市前金區中正四路211號。
    我的電話是0912345678，email是wang@example.com。
    我有在服用美沙冬，之前有憂鬱症病史。
    """
    
    # 執行脫敏
    result = redactor.redact(conversation, mode="mask", lang="zh-TW")
    
    print("原文：")
    print(conversation)
    print("\n脫敏後：")
    print(result.redacted)
    print(f"\n發現 {len(result.found_items)} 項敏感資訊：")
    
    for item in result.found_items:
        print(f"- {item['type']}: {item['category']} ({item['severity']})")
    
    # 測試批次處理
    batch_processor = BatchRedactor(redactor)
    texts = [conversation] * 10
    
    import asyncio
    results = asyncio.run(batch_processor.redact_batch(texts))
    print(f"\n批次處理 {len(results)} 筆資料完成")

if __name__ == "__main__":
    # 執行單元測試
    unittest.main()
    
    # 執行整合測試
    # integration_test()
```

## 效能優化建議

1. **快取編譯的正則表達式**
   - 預編譯所有 pattern
   - 使用 `re.compile()` 並重複使用

2. **批次處理**
   - 使用 ThreadPoolExecutor 平行處理
   - 大量資料時分批處理

3. **NER 模型優化**
   - 使用輕量級模型
   - 考慮使用 ONNX 加速
   - 實作模型快取

4. **規則優先級**
   - 高頻規則優先檢查
   - 簡單規則先於複雜規則

## 關鍵記憶點
1. **必須**同時使用規則和 NER 雙層檢測
2. **記得**保持假名化的一致性
3. **注意**不同語言需要不同的處理策略
4. **重要**定期更新脫敏規則庫